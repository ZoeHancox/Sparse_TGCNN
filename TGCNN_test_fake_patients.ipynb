{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4609f696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.694837Z",
     "start_time": "2022-12-20T15:00:03.051041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import keras\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from src import utils, trnvaltst_sigmoid_oned, TGCNN_layer, whole_model, whole_model_demographics, create_fake_patients, plot_figures\n",
    "from early_stopping import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, roc_auc_score, recall_score\n",
    "from csv import writer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "print(\"tensorflow version:\", tf. __version__)\n",
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f64909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.709798Z",
     "start_time": "2022-12-20T15:00:24.696832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_LSTM = False # True = without LSTM\n",
    "exponential_scaling = True # True = with exponential\n",
    "L1_ablation = True # True = with L1 reg\n",
    "L2_ablation = True # True = with L2 reg\n",
    "variable_gamma = True\n",
    "graph_reg_incl = True\n",
    "\n",
    "num_of_runs = 5\n",
    "    \n",
    "weighted_loss = False # class weighted to deal with imbalance if True\n",
    "no_timestamp = False # if no_timestamp = True then all values in 3-tensor = 1\n",
    "activation_type = 'LeakyReLU' #'relu','gelu', 'LeakyReLU'\n",
    "second_TGCNN_layer = True\n",
    "demo = True\n",
    "include_drugs = True\n",
    "\n",
    "run_name='hip_1999_to_one_year_advance_model_with_prescripts'\n",
    "\n",
    "# strings for hyperparameter searching file\n",
    "LSTM_str=\"LSTM excluded\" if no_LSTM == True else \"LSTM included\"\n",
    "exp_str = \"exp excluded\" if exponential_scaling == False else \"exp included\"\n",
    "timestamp_str = \"time elapsed = 1\" if no_timestamp == True else \"time elapsed\"\n",
    "weighted_loss_str = \"weighted_loss\" if weighted_loss ==True else \"unweighted_loss\"\n",
    "L1_str = \"L1 included\" if L1_ablation == True else \"L1 excluded\"\n",
    "L2_str = \"L2 included\" if L2_ablation == True else \"L2 excluded\"\n",
    "second_layer_str = \"Branched model\" if second_TGCNN_layer == True else \"Unbranched model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3eb542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>indices</th>\n",
       "      <th>values</th>\n",
       "      <th>num_time_steps</th>\n",
       "      <th>gender</th>\n",
       "      <th>imd_quin</th>\n",
       "      <th>age_at_label_event</th>\n",
       "      <th>replace_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[409, 41, 307], [102, 310, 495], [204, 453, 4...</td>\n",
       "      <td>[0.45496927826490074, 0.5810061483059896, 0.65...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[[286, 100, 22], [365, 132, 259], [41, 64, 353...</td>\n",
       "      <td>[0.7391977318803096, 0.6089012711422633, 0.155...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[[323, 215, 161], [218, 365, 356], [246, 98, 3...</td>\n",
       "      <td>[0.5295845027490297, 0.578570228590578, 0.3465...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[110, 271, 300], [251, 388, 16], [125, 370, 1...</td>\n",
       "      <td>[0.8080120518959206, 0.9516766139738275, 0.213...</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[[341, 234, 485], [29, 194, 460], [1, 184, 231...</td>\n",
       "      <td>[0.6940562077123074, 0.918016551432922, 0.9293...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>3995</td>\n",
       "      <td>[[262, 421, 168], [490, 362, 57], [286, 304, 5...</td>\n",
       "      <td>[0.5192050599850175, 0.35942831064409275, 0.92...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3996</td>\n",
       "      <td>[[80, 77, 231], [306, 195, 106], [478, 200, 10...</td>\n",
       "      <td>[0.6306261018473536, 0.2553058156250584, 0.201...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3997</td>\n",
       "      <td>[[315, 230, 243], [274, 334, 120], [374, 202, ...</td>\n",
       "      <td>[0.07180787154902091, 0.9924445874014004, 0.30...</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3998</td>\n",
       "      <td>[[473, 136, 363], [186, 451, 256], [308, 10, 2...</td>\n",
       "      <td>[0.880116448238413, 0.6303630121346312, 0.7590...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3999</td>\n",
       "      <td>[[289, 410, 147], [279, 32, 10], [87, 422, 364...</td>\n",
       "      <td>[0.9885490332458151, 0.7756500651037331, 0.348...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3999 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user                                            indices  \\\n",
       "0        1  [[409, 41, 307], [102, 310, 495], [204, 453, 4...   \n",
       "1        2  [[286, 100, 22], [365, 132, 259], [41, 64, 353...   \n",
       "2        3  [[323, 215, 161], [218, 365, 356], [246, 98, 3...   \n",
       "3        4  [[110, 271, 300], [251, 388, 16], [125, 370, 1...   \n",
       "4        5  [[341, 234, 485], [29, 194, 460], [1, 184, 231...   \n",
       "...    ...                                                ...   \n",
       "3994  3995  [[262, 421, 168], [490, 362, 57], [286, 304, 5...   \n",
       "3995  3996  [[80, 77, 231], [306, 195, 106], [478, 200, 10...   \n",
       "3996  3997  [[315, 230, 243], [274, 334, 120], [374, 202, ...   \n",
       "3997  3998  [[473, 136, 363], [186, 451, 256], [308, 10, 2...   \n",
       "3998  3999  [[289, 410, 147], [279, 32, 10], [87, 422, 364...   \n",
       "\n",
       "                                                 values  num_time_steps  \\\n",
       "0     [0.45496927826490074, 0.5810061483059896, 0.65...              14   \n",
       "1     [0.7391977318803096, 0.6089012711422633, 0.155...              52   \n",
       "2     [0.5295845027490297, 0.578570228590578, 0.3465...              40   \n",
       "3     [0.8080120518959206, 0.9516766139738275, 0.213...              67   \n",
       "4     [0.6940562077123074, 0.918016551432922, 0.9293...              30   \n",
       "...                                                 ...             ...   \n",
       "3994  [0.5192050599850175, 0.35942831064409275, 0.92...              61   \n",
       "3995  [0.6306261018473536, 0.2553058156250584, 0.201...              34   \n",
       "3996  [0.07180787154902091, 0.9924445874014004, 0.30...              77   \n",
       "3997  [0.880116448238413, 0.6303630121346312, 0.7590...              41   \n",
       "3998  [0.9885490332458151, 0.7756500651037331, 0.348...              12   \n",
       "\n",
       "      gender  imd_quin  age_at_label_event replace_type  \n",
       "0          0       3.0                70.0          hip  \n",
       "1          0       2.0                45.0         none  \n",
       "2          0       2.0                43.0          hip  \n",
       "3          1       1.0                71.0          hip  \n",
       "4          1       5.0                58.0         none  \n",
       "...      ...       ...                 ...          ...  \n",
       "3994       1       4.0                41.0         none  \n",
       "3995       1       1.0                49.0         none  \n",
       "3996       1       4.0                75.0         none  \n",
       "3997       0       4.0                63.0         none  \n",
       "3998       0       4.0                71.0         none  \n",
       "\n",
       "[3999 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if include_drugs:\n",
    "    max_event_codes = 518\n",
    "else:\n",
    "    max_event_codes = 512\n",
    "num_labels = 1 # number of outcomes -1 \n",
    "hip_or_knee = 'hip'\n",
    "class_weights = tf.compat.v2.constant([[0.5, 0.5]]) # 50:50 split for the class weights\n",
    "\n",
    "# This would be where you would import your data instead of using the `create_fake_patient_df` function\n",
    "    # Data to train the model on \n",
    "cv_patients = create_fake_patients.create_fake_patient_df(num_patients=4000, max_events=100, max_nodes=512)\n",
    "    # Data to test and relicibrate on\n",
    "test_patients = create_fake_patients.create_fake_patient_df(num_patients=4000, max_events=100, max_nodes=512)\n",
    "    # Data to test the recalibrated model on\n",
    "recal_test_patients = create_fake_patients.create_fake_patient_df(num_patients=4000, max_events=100, max_nodes=512)\n",
    "\n",
    "cv_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c8eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.080921Z",
     "start_time": "2022-12-20T15:02:15.672664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_matrices = []\n",
    "\n",
    "\n",
    "for patient in range(sample_size):\n",
    "#for patient in range(10240):\n",
    "    \n",
    "    i_list = input_values_indices_df.iloc[patient]['indices'] # indices from patient cell\n",
    "    #print(len(i_list))\n",
    "    v_list = input_values_indices_df.iloc[patient]['values'] # values from patient cell\n",
    "    #print(len(v_list))\n",
    "    \n",
    "    individual_sparse = tf.sparse.SparseTensor(i_list, v_list, (max_event_codes, max_event_codes, max_timesteps))\n",
    "    #print(individual_sparse)\n",
    "    \n",
    "    # adding the sparse tensor to a list of all the tensors\n",
    "    ordered_indiv = tf.sparse.reorder(individual_sparse) # reorder required for tensor to work (no effect to outcome)\n",
    "    input_matrices.append(tf.sparse.expand_dims(ordered_indiv, axis=0))\n",
    "    #print(input_matrices)\n",
    "    \n",
    "    if patient%10000 == 0:\n",
    "        print(f\"{patient}/{sample_size} converted to SparseTensors {patient/sample_size:.2%}\")\n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72e70c1d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb920bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.095923Z",
     "start_time": "2022-12-20T15:04:46.082885Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for cross validation the data could be split into 5 folds each which make up 20% of the data\n",
    "# but for now just split into train, validation and test\n",
    "train_perc = 0.7\n",
    "val_perc = 0.15\n",
    "test_perc = 0.15\n",
    "\n",
    "train_size = int(round(sample_size*train_perc))\n",
    "val_size = int(round(sample_size*val_perc))\n",
    "test_size = int(round(sample_size*test_perc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d4bb951",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6e878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.111881Z",
     "start_time": "2022-12-20T15:04:46.096918Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the average epoch to the list of all epochs\n",
    "def metric_save(trn_epoch_metric, trn_all_epoch_avgs, \n",
    "            val_epoch_metric, val_all_epoch_avgs,\n",
    "            test_epoch_metric, test_all_epoch_avgs):\n",
    "    trn_all_epoch_avgs.append(trn_epoch_metric)\n",
    "    val_all_epoch_avgs.append(val_epoch_metric)\n",
    "    test_all_epoch_avgs.append(test_epoch_metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72877dff",
   "metadata": {},
   "source": [
    "# Loop for hyperparameter tuning starts here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d0756d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Repeated Hyperparameter Selection - not random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b4c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.127968Z",
     "start_time": "2022-12-20T15:04:46.112875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    ############################# Repeated Hyperparameter Selection #############################\n",
    "#     nepochs = 20\n",
    "#     lr = 1e-2 #initial learning rate\n",
    "#     out_chans = 1 # number of filters\n",
    "#     filter_size = 3 # number of time steps/ filter size\n",
    "#     if no_LSTM:\n",
    "#         lstm_h = 0\n",
    "#     else:\n",
    "#         lstm_h = 128 # number of LSTM neurons\n",
    "    \n",
    "#     reg_strength = 1e-1#1e3\n",
    "#     linear_size = 512 # linear layer output size\n",
    "#     drop_val = 0.5\n",
    "    ############################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c12b99e2",
   "metadata": {},
   "source": [
    "### Random Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14366ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.699086Z",
     "start_time": "2022-12-20T15:04:46.128962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ran_search_num in range(num_of_runs):\n",
    "    #random.seed(time.time())\n",
    "    print('Run name:', run_name, ran_search_num)\n",
    "    indices = list(range(len(input_matrices)))\n",
    "    random.shuffle(indices) # shuffles datasets\n",
    "    train_set_indices = indices[:train_size]\n",
    "    val_set_indices = indices[train_size:train_size+val_size]\n",
    "    test_set_indices = indices[-test_size:]\n",
    "\n",
    "    train_batch_size = 1024\n",
    "    val_batch_size = 1024\n",
    "    test_batch_size = 1024\n",
    "#     print(\"y is\")\n",
    "#     print(y)\n",
    "#     print(type(y))\n",
    "#     print(type(train_set_indices))\n",
    "#     print(y.values.tolist())\n",
    "    batched_graphs_trn, batched_labels_trn = utils.batch_set(indice_set=train_set_indices, input_matrices=input_matrices, \n",
    "                                                       labels=y.to_numpy(), batchsize=train_batch_size)                                           \n",
    "    batched_graphs_val, batched_labels_val = utils.batch_set(indice_set=val_set_indices, input_matrices=input_matrices, \n",
    "                                                       labels=y.to_numpy(), batchsize=val_batch_size)\n",
    "    batched_graphs_test, batched_labels_test = utils.batch_set(indice_set=test_set_indices, input_matrices=input_matrices, \n",
    "                                                         labels=y.to_numpy(), batchsize=test_batch_size)\n",
    "    \n",
    "\n",
    "    #nepochs = random.choice([25, 50, 75, 100])\n",
    "    nepochs=2 #200\n",
    "    lr = random.choice([0.01, 0.05, 0.001, 0.005, 0.0001]) # initial learning rate\n",
    "\n",
    "    out_chans = random.choice([8, 16, 32]) # number of filters\n",
    "    filter_size = random.choice([3, 4, 6]) # number of time steps/ filter size\n",
    "    if no_LSTM:\n",
    "        lstm_h = 0\n",
    "    else:\n",
    "        lstm_h = random.choice([16, 32, 64, 128, 256]) # number of LSTM neurons\n",
    "    linear_size = random.choice([64, 128]) # linear layer output size\n",
    "    #drop_val = random.choice([0.2, 0.3, 0.4, 0.5])\n",
    "    drop_val = random.choice([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    reg_strength = random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\n",
    "    #reg_strength = random.choice([100, 10, 1, 1e-1, 5e-1])\n",
    "    graph_reg_strength = 1e1\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Number of 3D CNN filters: {out_chans}\")\n",
    "    print(f\"Filter size: {filter_size}\")\n",
    "    print(f\"Number of LSTM neurons: {lstm_h}\")\n",
    "    print(f\"Number of fully connected layers: {linear_size}\")\n",
    "    print(f\"Dropout value: {drop_val}\")\n",
    "    print(f\"Regularisation strength: {reg_strength}\")\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(\"*\"*40)\n",
    "    model = whole_model.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                        filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                        exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                       fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                        no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    #cce_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    epochs = nepochs\n",
    "    train_loss_epochs, train_acc_epochs, train_prec_epochs, train_recall_epochs, train_f1_epochs, train_auc_epochs = [],[],[],[],[],[]\n",
    "    train_indiv_acc_epochs, train_indiv_prec_epochs, train_indiv_recall_epochs, train_indiv_f1_epochs, train_indiv_auc_epochs = [],[],[],[],[]\n",
    "    val_loss_epochs, val_acc_epochs, val_prec_epochs, val_recall_epochs, val_f1_epochs, val_auc_epochs = [],[],[],[],[],[]\n",
    "    val_indiv_acc_epochs, val_indiv_prec_epochs, val_indiv_recall_epochs, val_indiv_f1_epochs, val_indiv_auc_epochs = [],[],[],[],[]\n",
    "    test_loss_epochs, test_acc_epochs, test_prec_epochs, test_recall_epochs, test_f1_epochs, test_auc_epochs = [],[],[],[],[],[]\n",
    "    test_indiv_acc_epochs, test_indiv_prec_epochs, test_indiv_recall_epochs, test_indiv_f1_epochs, test_indiv_auc_epochs = [],[],[],[],[]\n",
    "    for epoch in range(epochs):\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}/{nepochs}\")\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        train_loss_list, train_acc_list, train_prec_list, train_recall_list, train_f1_list, train_auc_list = [],[],[],[],[],[]\n",
    "        #train_all_classes_acc_list, \n",
    "        train_all_classes_prec_list, train_all_classes_recall_list, train_all_classes_f1_list, train_all_classes_auc_list = [],[],[],[] # lists of lists\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(zip(batched_graphs_trn, batched_labels_trn)):\n",
    "            y_batch_train=np.array(y_batch_train)\n",
    "            trn_logits, trn_loss, trn_acc, trn_prec, trn_recall, trn_auc, trn_f1, \\\n",
    "            indiv_trn_prec, indiv_trn_recall, indiv_trn_auc, indiv_trn_f1, \\\n",
    "            = trnvaltst.train_step(x_batch_train,y_batch_train,reg_strength,class_weights,model,L1_ablation,L2_ablation, \n",
    "                                   graph_reg_strength, graph_reg_incl,\n",
    "                                   weighted_loss, variable_gamma, optimizer)\n",
    "            \n",
    "            train_loss_list.append(trn_loss)\n",
    "            train_acc_list.append(trn_acc)\n",
    "        \n",
    "            train_all_classes_prec_list.append(trn_prec)\n",
    "            train_prec_list.append(indiv_trn_prec)\n",
    "\n",
    "            train_all_classes_recall_list.append(trn_recall)\n",
    "            train_recall_list.append(indiv_trn_recall)\n",
    "            \n",
    "            train_all_classes_auc_list.append(trn_auc)\n",
    "            train_auc_list.append(indiv_trn_auc)\n",
    "            \n",
    "            train_all_classes_f1_list.append(trn_f1)\n",
    "            train_f1_list.append(indiv_trn_f1)\n",
    "\n",
    "        \n",
    "        # get the average metric score for each class for this ONE epoch using the batch list metrics\n",
    "        #train_acc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_acc_list) # average accuracy score for each class over one epoch\n",
    "        train_prec_indiv_ave = utils.average_of_list_of_lists(train_all_classes_prec_list)\n",
    "        train_recall_indiv_ave = utils.average_of_list_of_lists(train_all_classes_recall_list)\n",
    "        train_f1_indiv_ave = utils.average_of_list_of_lists(train_all_classes_f1_list) # ave F1 score for each class\n",
    "        train_auc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_auc_list)\n",
    "        \n",
    "        print(\"\\nTRAINING METRICS:\")\n",
    "#         print(f\"Training individual precision scores: {train_prec_indiv_ave}\")\n",
    "#         print(f\"Training individual recall scores: {train_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Training individual F1 scores: {train_f1_indiv_ave}\")\n",
    "#         print(f\"Training individual AUC scores: {train_auc_indiv_ave}\")\n",
    "        print(f\"Train loss {np.mean(train_loss_list):.4f}\")\n",
    "        print(f\"Train accuracy: {np.mean(train_acc_list) :.4%}\")\n",
    "        \n",
    " \n",
    "    \n",
    "\n",
    "        # Validation loop\n",
    "        val_loss_list, val_acc_list,  val_prec_list, val_recall_list, val_auc_list, val_f1_list = [],[],[],[],[],[]\n",
    "        val_all_classes_acc_list, val_all_classes_prec_list, val_all_classes_recall_list, val_all_classes_f1_list, val_all_classes_auc_list = [],[],[],[],[]\n",
    "        for x_batch_val, y_batch_val in zip(batched_graphs_val, batched_labels_val):\n",
    "            val_logits, val_loss, val_acc, val_prec, val_recall, val_auc, val_f1, \\\n",
    "            indiv_val_prec, indiv_val_recall, indiv_val_auc, indiv_val_f1, \\\n",
    "            = trnvaltst.val_step(x_batch_val,y_batch_val,reg_strength,class_weights,model,L1_ablation,weighted_loss)\n",
    "\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            val_acc_list.append(val_acc)\n",
    "            \n",
    "            val_prec_list.append(val_prec)\n",
    "            val_all_classes_prec_list.append(indiv_val_prec)\n",
    "            \n",
    "            val_recall_list.append(val_recall) \n",
    "            val_all_classes_recall_list.append(indiv_val_recall)\n",
    "            \n",
    "            val_auc_list.append(val_auc)\n",
    "            val_all_classes_auc_list.append(indiv_val_auc)\n",
    "            \n",
    "            val_f1_list.append(val_f1)\n",
    "            val_all_classes_f1_list.append(indiv_val_f1)\n",
    "            \n",
    "        val_prec_indiv_ave = utils.average_of_list_of_lists(val_all_classes_prec_list)\n",
    "        val_recall_indiv_ave = utils.average_of_list_of_lists(val_all_classes_recall_list)\n",
    "        val_f1_indiv_ave = utils.average_of_list_of_lists(val_all_classes_f1_list) # ave F1 score for each class\n",
    "        val_auc_indiv_ave = utils.average_of_list_of_lists(val_all_classes_auc_list)\n",
    "\n",
    "        print(\"\\nVALIDATION METRICS:\")\n",
    "#         print(f\"Validation individual precision scores: {val_prec_indiv_ave}\")\n",
    "#         print(f\"Validation individual recall scores: {val_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Validation individual F1 scores: {val_f1_indiv_ave}\")\n",
    "#         print(f\"Validation individual AUC scores: {val_auc_indiv_ave}\")\n",
    "        print(f\"Validation loss: {np.mean(val_loss_list):.4f}\")\n",
    "        print(f\"Validation accuracy: {np.mean(val_acc_list) :.4%}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Test loop\n",
    "        y_batch_test_list_of_lists = []\n",
    "        test_logits_list, test_loss_list, test_acc_list, test_prec_list, test_recall_list, test_auc_list, test_f1_list = [], [],[],[],[],[],[]\n",
    "        train_all_classes_acc_list, test_all_classes_prec_list, test_all_classes_recall_list, test_all_classes_f1_list, test_all_classes_auc_list = [],[],[],[],[]\n",
    "        for x_batch_test, y_batch_test in zip(batched_graphs_test, batched_labels_test):\n",
    "            \n",
    "            test_logits, test_loss, test_acc, test_prec, test_recall, test_auc, test_f1, \\\n",
    "            indiv_test_prec, indiv_test_recall, indiv_test_auc, indiv_test_f1, \\\n",
    "            = trnvaltst.val_step(x_batch_test,y_batch_test,reg_strength,\n",
    "                                        class_weights,model,L1_ablation,L2_ablation,\n",
    "                                         graph_reg_strength, graph_reg_incl,weighted_loss)\n",
    "            \n",
    "\n",
    "            test_logits_list.append(test_logits)\n",
    "            y_batch_test_list_of_lists.append(y_batch_test)\n",
    "            test_loss_list.append(test_loss)\n",
    "            test_acc_list.append(test_acc)\n",
    "            test_prec_list.append(test_prec)\n",
    "            test_all_classes_prec_list.append(indiv_test_prec)\n",
    "            \n",
    "            test_recall_list.append(test_recall) \n",
    "            test_all_classes_recall_list.append(indiv_test_recall)\n",
    "            \n",
    "            test_auc_list.append(test_auc)\n",
    "            test_all_classes_auc_list.append(indiv_test_auc)\n",
    "            \n",
    "            test_f1_list.append(test_f1)\n",
    "            test_all_classes_f1_list.append(indiv_test_f1)\n",
    "            \n",
    "            \n",
    "        test_prec_indiv_ave = utils.average_of_list_of_lists(test_all_classes_prec_list)\n",
    "        test_recall_indiv_ave = utils.average_of_list_of_lists(test_all_classes_recall_list)\n",
    "        test_f1_indiv_ave = utils.average_of_list_of_lists(test_all_classes_f1_list) # ave F1 score for each class\n",
    "        test_auc_indiv_ave = utils.average_of_list_of_lists(test_all_classes_auc_list)        \n",
    "        print(\"\\nTEST METRICS:\")\n",
    "#         print(f\"Test individual precision scores: {test_prec_indiv_ave}\")\n",
    "#         print(f\"Test individual recall scores: {test_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Test individual F1 scores: {test_f1_indiv_ave}\")\n",
    "#         print(f\"Test individual AUC scores: {test_auc_indiv_ave}\")\n",
    "        print(f\"Test loss: {np.mean(test_loss_list):.4f}\") # average loss from the epoch\n",
    "        print(f\"Test accuracy: {np.mean(test_acc_list) :.4%}\")\n",
    "#         print(f\"Test precision: {np.mean(test_prec_list) :.4f}\")\n",
    "#         print(f\"Test recall: {np.mean(test_recall_list) :.4f}\")\n",
    "\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # get the average metric from ONE epoch\n",
    "        ave_epoch_train_loss, ave_epoch_train_acc, ave_epoch_train_auc, ave_epoch_train_prec, ave_epoch_train_recall, ave_epoch_train_f1 = np.mean(train_loss_list), np.mean(train_acc_list), np.mean(train_auc_list), np.mean(train_prec_list), np.mean(train_recall_list), np.mean(train_f1_list)\n",
    "        ave_epoch_val_loss, ave_epoch_val_acc, ave_epoch_val_auc, ave_epoch_val_prec, ave_epoch_val_recall, ave_epoch_val_f1 = np.mean(val_loss_list), np.mean(val_acc_list), np.mean(val_auc_list), np.mean(val_prec_list), np.mean(val_recall_list), np.mean(val_f1_list)\n",
    "        ave_epoch_test_loss, ave_epoch_test_acc, ave_epoch_test_auc, ave_epoch_test_prec, ave_epoch_test_recall, ave_epoch_test_f1 = np.mean(test_loss_list), np.mean(test_acc_list), np.mean(val_auc_list), np.mean(test_prec_list), np.mean(test_recall_list), np.mean(test_f1_list)\n",
    "\n",
    "\n",
    "        #print(optimizer.get_config())\n",
    "\n",
    "        # save the average (from one epoch) to the list of ALL epochs\n",
    "        metric_save(ave_epoch_train_loss, train_loss_epochs, ave_epoch_val_loss, val_loss_epochs, \n",
    "                    ave_epoch_test_loss, test_loss_epochs) #losses\n",
    "        metric_save(ave_epoch_train_acc, train_acc_epochs, ave_epoch_val_acc, val_acc_epochs, \n",
    "                    ave_epoch_test_acc, test_acc_epochs) # acc\n",
    "        metric_save(ave_epoch_train_auc, train_auc_epochs, ave_epoch_val_auc, val_auc_epochs, \n",
    "                    ave_epoch_test_auc, test_auc_epochs) # auc        \n",
    "        metric_save(ave_epoch_train_prec, train_prec_epochs, ave_epoch_val_prec, val_prec_epochs, \n",
    "                    ave_epoch_test_prec, test_prec_epochs) # prec\n",
    "        metric_save(ave_epoch_train_recall, train_recall_epochs, ave_epoch_val_recall, \n",
    "                    val_recall_epochs, ave_epoch_test_recall, test_recall_epochs) # recall\n",
    "        metric_save(ave_epoch_train_f1, train_f1_epochs, ave_epoch_val_f1, \n",
    "                    val_f1_epochs, ave_epoch_test_f1, test_f1_epochs)\n",
    "        \n",
    "        ################## save the average from one epoch to the list of ALL epochs for each individual class\n",
    "        metric_save(train_auc_indiv_ave, train_indiv_auc_epochs, val_auc_indiv_ave, val_indiv_auc_epochs,\n",
    "                   test_auc_indiv_ave, test_indiv_auc_epochs)\n",
    "        metric_save(train_prec_indiv_ave, train_indiv_prec_epochs, val_prec_indiv_ave, val_indiv_prec_epochs,\n",
    "                   test_prec_indiv_ave, test_indiv_prec_epochs)\n",
    "        metric_save(train_recall_indiv_ave, train_indiv_recall_epochs, val_recall_indiv_ave, val_indiv_recall_epochs,\n",
    "                   test_recall_indiv_ave, test_indiv_recall_epochs)\n",
    "        metric_save(train_f1_indiv_ave, train_indiv_f1_epochs, val_f1_indiv_ave, val_indiv_f1_epochs,\n",
    "                   test_f1_indiv_ave, test_indiv_f1_epochs)\n",
    "\n",
    "        \n",
    "        early_stopping_metric = val_loss_list # metric that is used to determine if the model should stop training\n",
    "        early_stopping(np.mean(early_stopping_metric), train_loss_list, val_loss_list, \n",
    "                       test_loss_list, train_acc_list, val_acc_list, \n",
    "                       test_acc_list, \n",
    "                       train_auc_list, val_auc_list, test_auc_list,\n",
    "                       train_auc_indiv_ave, val_auc_indiv_ave, test_auc_indiv_ave,\n",
    "                       \n",
    "                       train_prec_list, val_prec_list, test_prec_list,\n",
    "                       train_prec_indiv_ave, val_prec_indiv_ave, test_prec_indiv_ave,\n",
    "                       \n",
    "                       train_recall_list, val_recall_list, test_recall_list,\n",
    "                       train_recall_indiv_ave, val_recall_indiv_ave, test_recall_indiv_ave,\n",
    "                       \n",
    "                       train_f1_list, val_f1_list, test_f1_list,\n",
    "                       train_f1_indiv_ave, val_f1_indiv_ave, test_f1_indiv_ave\n",
    "                      )\n",
    "\n",
    "        if early_stopping.checkpoint_made:\n",
    "            \n",
    "            test_logits = tf.concat(test_logits_list, axis=0)\n",
    "            y_batch_test_list = [item for sublist in y_batch_test_list_of_lists for item in sublist]\n",
    "            \n",
    "            checkpoint_train_loss, checkpoint_val_loss, checkpoint_test_loss, \\\n",
    "            checkpoint_train_acc, checkpoint_val_acc,  checkpoint_test_acc, \\\n",
    "            checkpoint_train_auc, checkpoint_val_auc,  checkpoint_test_auc, \\\n",
    "            checkpoint_train_auc_indiv, checkpoint_val_auc_indiv, checkpoint_test_auc_indiv, \\\n",
    "            checkpoint_train_prec, checkpoint_val_prec, checkpoint_test_prec,  \\\n",
    "            checkpoint_train_prec_indiv, checkpoint_val_prec_indiv, checkpoint_test_prec_indiv, \\\n",
    "            checkpoint_train_recall, checkpoint_val_recall, checkpoint_test_recall, \\\n",
    "            checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, checkpoint_test_recall_indiv, \\\n",
    "            checkpoint_train_f1, checkpoint_val_f1, checkpoint_test_f1, \\\n",
    "            checkpoint_train_f1_indiv, checkpoint_val_f1_indiv, checkpoint_test_f1_indiv, \\\n",
    "            checkpoint_y_test, checkpoint_logits_test = early_stopping.print_checkpoint_metric(train_loss_list, \n",
    "                                                                        val_loss_list, test_loss_list, train_acc_list, val_acc_list, \n",
    "                                                                        test_acc_list, train_auc_list, val_auc_list, test_auc_list, \n",
    "                                                                        train_auc_indiv_ave, val_auc_indiv_ave, test_auc_indiv_ave,\n",
    "                                                                        train_prec_list, val_prec_list, test_prec_list,\n",
    "                                                                        train_prec_indiv_ave, val_prec_indiv_ave, test_prec_indiv_ave,                                        \n",
    "                                                                        train_recall_list, val_recall_list, test_recall_list,\n",
    "                                                                        train_recall_indiv_ave, val_recall_indiv_ave, test_recall_indiv_ave,                                        \n",
    "                                                                        train_f1_list, val_f1_list, test_f1_list,\n",
    "                                                                        train_f1_indiv_ave, val_f1_indiv_ave, test_f1_indiv_ave,\n",
    "                                                                        y_batch_test_list, test_logits)\n",
    "\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if variable_gamma:\n",
    "        gamma_val = float(model.tg_conv_layer1.gammat.numpy()) # gamma doesn't seem to be training correctly atm\n",
    "        print(gamma_val)\n",
    "    else:\n",
    "        gamma_val = 'N/A'\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    time_taken = timedelta(seconds=end_time - start_time)\n",
    "     \n",
    "\n",
    "        \n",
    "    plot_figures.plot_loss_curve(train_loss = train_loss_epochs, val_loss = val_loss_epochs, test_lost = test_loss_epochs, \n",
    "                    run_name=None, ran_search_num=ran_search_num)\n",
    "\n",
    "    plot_figures.draw_confusion_mat(checkpoint_y_test, checkpoint_logits_test, class_names, None, ran_search_num)\n",
    "    plot_figures.draw_calibration_curves(checkpoint_y_test, checkpoint_logits_test, class_names, None, ran_search_num)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab92f56",
   "metadata": {},
   "source": [
    "`WARNING:tensorflow:Gradients do not exist for variables ['3DCNN_Weights:0'] when minimizing the loss. If you're using 'model.compile()', did you forget to provide a 'loss' argument?` error is caused by one of the conv_layers not being used for a single stream model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e94a2bb0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Printing logits and model summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8902aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.716378Z",
     "start_time": "2022-12-20T15:08:41.716378Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be4292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.717373Z",
     "start_time": "2022-12-20T15:08:41.717373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984340a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.721359Z",
     "start_time": "2022-12-20T15:08:41.721359Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits_np = test_logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091085a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.722355Z",
     "start_time": "2022-12-20T15:08:41.722355Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(logits_np, columns=['High', 'Low', 'Zero'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c0f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.722355Z",
     "start_time": "2022-12-20T15:08:41.722355Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Max'] = df.idxmax(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e77d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.723365Z",
     "start_time": "2022-12-20T15:08:41.723365Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Max.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1b6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "483.316px",
    "left": "1447.26px",
    "right": "20px",
    "top": "158.896px",
    "width": "529.462px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
