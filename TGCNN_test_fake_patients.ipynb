{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4609f696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.694837Z",
     "start_time": "2022-12-20T15:00:03.051041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import keras\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from src import utils, trnvaltst_sigmoid_oned, TGCNN_layer, whole_model, whole_model_demographics, create_fake_patients, plot_figures\n",
    "from early_stopping_cv import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, roc_auc_score, recall_score\n",
    "from csv import writer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "print(\"tensorflow version:\", tf. __version__)\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f64909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.709798Z",
     "start_time": "2022-12-20T15:00:24.696832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_LSTM = False # True = without LSTM\n",
    "exponential_scaling = True # True = with exponential\n",
    "L1_ablation = True # True = with L1 reg\n",
    "L2_ablation = True # True = with L2 reg\n",
    "variable_gamma = True\n",
    "graph_reg_incl = True\n",
    "\n",
    "num_of_runs = 1\n",
    "    \n",
    "weighted_loss = False # class weighted to deal with imbalance if True\n",
    "no_timestamp = False # if no_timestamp = True then all values in 3-tensor = 1\n",
    "activation_type = 'LeakyReLU' #'relu','gelu', 'LeakyReLU'\n",
    "second_TGCNN_layer = True\n",
    "demo = True\n",
    "include_drugs = False\n",
    "\n",
    "run_name='hip_1999_to_one_year_advance_model'\n",
    "\n",
    "# strings for hyperparameter searching file\n",
    "LSTM_str=\"LSTM excluded\" if no_LSTM == True else \"LSTM included\"\n",
    "exp_str = \"exp excluded\" if exponential_scaling == False else \"exp included\"\n",
    "timestamp_str = \"time elapsed = 1\" if no_timestamp == True else \"time elapsed\"\n",
    "weighted_loss_str = \"weighted_loss\" if weighted_loss ==True else \"unweighted_loss\"\n",
    "L1_str = \"L1 included\" if L1_ablation == True else \"L1 excluded\"\n",
    "L2_str = \"L2 included\" if L2_ablation == True else \"L2 excluded\"\n",
    "second_layer_str = \"Branched model\" if second_TGCNN_layer == True else \"Unbranched model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3eb542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>indices</th>\n",
       "      <th>values</th>\n",
       "      <th>num_time_steps</th>\n",
       "      <th>gender</th>\n",
       "      <th>imd_quin</th>\n",
       "      <th>age_at_label_event</th>\n",
       "      <th>replace_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[133, 345, 100], [83, 133, 99], [371, 83, 98]]</td>\n",
       "      <td>[0.9113114191558278, 0.05307565824616445, 0.58...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[[83, 502, 100], [166, 83, 99], [460, 166, 98]...</td>\n",
       "      <td>[0.3364289464236956, 0.41151227918876, 0.50929...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[[92, 125, 100], [256, 92, 99], [91, 256, 98],...</td>\n",
       "      <td>[0.3409860742577857, 0.062314004221381225, 0.0...</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[163, 450, 100], [139, 163, 99], [353, 139, 9...</td>\n",
       "      <td>[0.22867635263719233, 0.9915140733519203, 0.94...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[[153, 187, 100], [475, 153, 99], [30, 475, 98...</td>\n",
       "      <td>[0.5120784845973885, 0.13160625498455802, 0.61...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>[[75, 236, 100], [5, 75, 99], [109, 5, 98], [1...</td>\n",
       "      <td>[0.9503452716785932, 0.1452950293502031, 0.807...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>[[225, 184, 100], [225, 225, 99], [192, 225, 9...</td>\n",
       "      <td>[0.25843717724112536, 0.744796147340657, 0.507...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>hip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>[[450, 139, 100], [509, 450, 99], [24, 509, 98...</td>\n",
       "      <td>[0.07580422158983624, 0.624306566758521, 0.213...</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>[[68, 357, 100], [487, 68, 99], [19, 487, 98],...</td>\n",
       "      <td>[0.9131785390290498, 0.4571655521409259, 0.497...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>[[323, 69, 100], [326, 323, 99], [3, 326, 98],...</td>\n",
       "      <td>[0.35494895382067526, 0.059038632239244104, 0....</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user                                            indices  \\\n",
       "0       1    [[133, 345, 100], [83, 133, 99], [371, 83, 98]]   \n",
       "1       2  [[83, 502, 100], [166, 83, 99], [460, 166, 98]...   \n",
       "2       3  [[92, 125, 100], [256, 92, 99], [91, 256, 98],...   \n",
       "3       4  [[163, 450, 100], [139, 163, 99], [353, 139, 9...   \n",
       "4       5  [[153, 187, 100], [475, 153, 99], [30, 475, 98...   \n",
       "..    ...                                                ...   \n",
       "494   495  [[75, 236, 100], [5, 75, 99], [109, 5, 98], [1...   \n",
       "495   496  [[225, 184, 100], [225, 225, 99], [192, 225, 9...   \n",
       "496   497  [[450, 139, 100], [509, 450, 99], [24, 509, 98...   \n",
       "497   498  [[68, 357, 100], [487, 68, 99], [19, 487, 98],...   \n",
       "498   499  [[323, 69, 100], [326, 323, 99], [3, 326, 98],...   \n",
       "\n",
       "                                                values  num_time_steps  \\\n",
       "0    [0.9113114191558278, 0.05307565824616445, 0.58...               3   \n",
       "1    [0.3364289464236956, 0.41151227918876, 0.50929...              11   \n",
       "2    [0.3409860742577857, 0.062314004221381225, 0.0...              66   \n",
       "3    [0.22867635263719233, 0.9915140733519203, 0.94...              30   \n",
       "4    [0.5120784845973885, 0.13160625498455802, 0.61...              23   \n",
       "..                                                 ...             ...   \n",
       "494  [0.9503452716785932, 0.1452950293502031, 0.807...              34   \n",
       "495  [0.25843717724112536, 0.744796147340657, 0.507...              12   \n",
       "496  [0.07580422158983624, 0.624306566758521, 0.213...              46   \n",
       "497  [0.9131785390290498, 0.4571655521409259, 0.497...              24   \n",
       "498  [0.35494895382067526, 0.059038632239244104, 0....              21   \n",
       "\n",
       "     gender  imd_quin  age_at_label_event replace_type  \n",
       "0         1       4.0                60.0         none  \n",
       "1         0       3.0                72.0         none  \n",
       "2         0       3.0                64.0         none  \n",
       "3         0       5.0                45.0         none  \n",
       "4         0       2.0                66.0         none  \n",
       "..      ...       ...                 ...          ...  \n",
       "494       1       5.0                88.0          hip  \n",
       "495       0       5.0                50.0          hip  \n",
       "496       1       4.0                49.0         none  \n",
       "497       0       4.0                86.0         none  \n",
       "498       0       2.0                65.0         none  \n",
       "\n",
       "[499 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if include_drugs:\n",
    "    max_event_codes = 518\n",
    "else:\n",
    "    max_event_codes = 512\n",
    "num_labels = 1 # number of outcomes -1 \n",
    "hip_or_knee = 'hip'\n",
    "class_weights = tf.compat.v2.constant([[0.5, 0.5]]) # 50:50 split for the class weights\n",
    "\n",
    "# This would be where you would import your data instead of using the `create_fake_patient_df` function\n",
    "    # Data to train the model on \n",
    "cv_patients = create_fake_patients.create_fake_patient_df(num_patients=500, max_events=100, max_nodes=512)\n",
    "    # Data to test and relicibrate on\n",
    "test_patients = create_fake_patients.create_fake_patient_df(num_patients=500, max_events=100, max_nodes=512)\n",
    "    # Data to test the recalibrated model on\n",
    "recal_test_patients = create_fake_patients.create_fake_patient_df(num_patients=500, max_events=100, max_nodes=512)\n",
    "\n",
    "cv_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bc6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sample_size = len(cv_patients)\n",
    "test_sample_size = len(test_patients)\n",
    "recal_test_sample_size = len(recal_test_patients)\n",
    "\n",
    "# NOTE: num_time_steps isn't actually the number of time_steps it's the length of the values list (sometimes multiple Read codes will be reported in one visit)\n",
    "# This is needed to create a sparse matrix\n",
    "\n",
    "max_timesteps_cv = cv_patients['num_time_steps'].max() \n",
    "max_timesteps_test = test_patients['num_time_steps'].max()\n",
    "max_timesteps_recal = recal_test_patients['num_time_steps'].max()\n",
    "max_timesteps = max(max_timesteps_cv, max_timesteps_test, max_timesteps_recal)\n",
    "max_timesteps\n",
    "max_timesteps = max_timesteps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566a6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in the cv input data 499\n",
      "Number of people in cv label data: 499\n",
      "Number of people in the test input data 499\n",
      "Number of people in test label data: 499\n",
      "Number of patients in each class in cv set:\n",
      "replace_type\n",
      "none    256\n",
      "hip     243\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of patients in each class in test set:\n",
      "replace_type\n",
      "hip     264\n",
      "none    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cv_y = utils.get_labels_binary(cv_patients)\n",
    "test_y = utils.get_labels_binary(test_patients)\n",
    "recal_test_y = utils.get_labels_binary(recal_test_patients)\n",
    "\n",
    "utils.check_group_sizes(cv_patients, test_patients, cv_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc65cc6",
   "metadata": {},
   "source": [
    "#### Convert list of lists of event codes into Sparse Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8082c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv : converting from input matrices to SparseTensors\n",
      "0/499 converted to SparseTensors 0.00%\n",
      "test : converting from input matrices to SparseTensors\n",
      "0/499 converted to SparseTensors 0.00%\n",
      "recal_test : converting from input matrices to SparseTensors\n",
      "0/499 converted to SparseTensors 0.00%\n"
     ]
    }
   ],
   "source": [
    "cv_input_matrices = utils.create_sparse_tensors(cv_sample_size, cv_patients, max_event_codes, max_timesteps, 'cv')\n",
    "test_input_matrices = utils.create_sparse_tensors(test_sample_size, test_patients, max_event_codes, max_timesteps, 'test')\n",
    "recal_test_input_matrices = utils.create_sparse_tensors(recal_test_sample_size, recal_test_patients, max_event_codes, max_timesteps, 'recal_test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72877dff",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning, model 5-fold-cv training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14366ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.699086Z",
     "start_time": "2022-12-20T15:04:46.128962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: hip_1999_to_one_year_advance_model 1\n",
      "Number of epochs: 1\n",
      "Learning rate: 0.0001\n",
      "Number of 3D CNN filters: 16\n",
      "Filter size: 3\n",
      "Number of LSTM neurons: 128\n",
      "Number of fully connected layers: 128\n",
      "Dropout value: 0.9\n",
      "Regularisation strength: 0.05\n",
      "****************************************\n",
      "hip_1999_to_one_year_advance_model_split1 _find\n",
      "normalised gamma [[-0.02515165]]\n",
      "normalised gamma [[-0.04172078]]\n",
      "\n",
      "Epoch 1/1\n",
      "Gamma: -0.025252198800444603\n",
      "Gamma 2: -0.04193536192178726\n",
      "\n",
      "TRAINING METRICS:\n",
      "Training AUC: 0.4976\n",
      "Train loss 25015.3477\n",
      "Train calibration slope 0.4197\n",
      "Train accuracy: 49.8737%\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Validation AUC: 0.5000\n",
      "Validation loss: 24858.8457\n",
      "Validation calibration slope: 0.5581\n",
      "Validation accuracy: 46.0000%\n",
      "==================================================\n",
      "Validation loss decreased (inf --> 24858.845703).\n",
      "\n",
      "TEST/HOLDOUT METRICS:\n",
      "Test individual AUC scores: 0.5000\n",
      "Test calibration: 0.5469\n",
      "Test accuracy: 47.1032%\n",
      "Test Recall: 0.4710\n",
      "Test Precision: 0.7525\n",
      "Test F1: 0.3027\n",
      "****************************************\n",
      "Gamma: -0.025252198800444603\n",
      "Gamma 2: -0.04193536192178726\n",
      "\n",
      "CHECKPOINTED RESULTS\n",
      "Train loss: 25015.3477\n",
      "Train AUC: 49.7607%\n",
      "\n",
      "Val loss: 24858.8457\n",
      "Val AUC: 50.0000%\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "hip_1999_to_one_year_advance_model_split2 _find\n",
      "normalised gamma [[0.03260758]]\n",
      "normalised gamma [[-0.02530439]]\n",
      "\n",
      "Epoch 1/1\n",
      "Gamma: 0.032988280057907104\n",
      "Gamma 2: -0.02554291859269142\n",
      "\n",
      "TRAINING METRICS:\n",
      "Training AUC: 0.4755\n",
      "Train loss 25012.3105\n",
      "Train calibration slope -0.9461\n",
      "Train accuracy: 47.3712%\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Validation AUC: 0.5403\n",
      "Validation loss: 24855.7969\n",
      "Validation calibration slope: 10.7107\n",
      "Validation accuracy: 56.0000%\n",
      "==================================================\n",
      "Validation loss decreased (inf --> 24855.796875).\n",
      "\n",
      "TEST/HOLDOUT METRICS:\n",
      "Test individual AUC scores: 0.4949\n",
      "Test calibration: -1.4457\n",
      "Test accuracy: 47.8952%\n",
      "Test Recall: 0.4790\n",
      "Test Precision: 0.4947\n",
      "Test F1: 0.4229\n",
      "****************************************\n",
      "Gamma: 0.032988280057907104\n",
      "Gamma 2: -0.02554291859269142\n",
      "\n",
      "CHECKPOINTED RESULTS\n",
      "Train loss: 25012.3105\n",
      "Train AUC: 47.5539%\n",
      "\n",
      "Val loss: 24855.7969\n",
      "Val AUC: 54.0345%\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "hip_1999_to_one_year_advance_model_split3 _find\n",
      "normalised gamma [[0.01450402]]\n",
      "normalised gamma [[-0.04263521]]\n",
      "\n",
      "Epoch 1/1\n",
      "Gamma: 0.014494501985609531\n",
      "Gamma 2: -0.04233739897608757\n",
      "\n",
      "TRAINING METRICS:\n",
      "Training AUC: 0.4650\n",
      "Train loss 25010.9062\n",
      "Train calibration slope 0.0796\n",
      "Train accuracy: 46.3561%\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Validation AUC: 0.5507\n",
      "Validation loss: 24854.3848\n",
      "Validation calibration slope: 12.0444\n",
      "Validation accuracy: 56.0000%\n",
      "==================================================\n",
      "Validation loss decreased (inf --> 24854.384766).\n",
      "\n",
      "TEST/HOLDOUT METRICS:\n",
      "Test individual AUC scores: 0.5305\n",
      "Test calibration: 7.1201\n",
      "Test accuracy: 52.3194%\n",
      "Test Recall: 0.5232\n",
      "Test Precision: 0.5363\n",
      "Test F1: 0.5201\n",
      "****************************************\n",
      "Gamma: 0.014494501985609531\n",
      "Gamma 2: -0.04233739897608757\n",
      "\n",
      "CHECKPOINTED RESULTS\n",
      "Train loss: 25010.9062\n",
      "Train AUC: 46.4997%\n",
      "\n",
      "Val loss: 24854.3848\n",
      "Val AUC: 55.0725%\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "hip_1999_to_one_year_advance_model_split4 _find\n",
      "normalised gamma [[-0.07600118]]\n",
      "normalised gamma [[0.01209029]]\n",
      "\n",
      "Epoch 1/1\n",
      "Gamma: -0.0758548378944397\n",
      "Gamma 2: 0.012103425338864326\n",
      "\n",
      "TRAINING METRICS:\n",
      "Training AUC: 0.5212\n",
      "Train loss 25010.5430\n",
      "Train calibration slope 1.3184\n",
      "Train accuracy: 51.6364%\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Validation AUC: 0.4903\n",
      "Validation loss: 24854.0410\n",
      "Validation calibration slope: -0.9956\n",
      "Validation accuracy: 50.0000%\n",
      "==================================================\n",
      "Validation loss decreased (inf --> 24854.041016).\n",
      "Gamma: -0.0758548378944397\n",
      "Gamma 2: 0.012103425338864326\n",
      "\n",
      "CHECKPOINTED RESULTS\n",
      "Train loss: 25010.5430\n",
      "Train AUC: 52.1185%\n",
      "\n",
      "Val loss: 24854.0410\n",
      "Val AUC: 49.0338%\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "hip_1999_to_one_year_advance_model_split5 _find\n",
      "normalised gamma [[0.02413989]]\n",
      "normalised gamma [[-0.01307582]]\n",
      "\n",
      "Epoch 1/1\n",
      "Gamma: 0.023880165070295334\n",
      "Gamma 2: -0.013393701985478401\n",
      "\n",
      "TRAINING METRICS:\n",
      "Training AUC: 0.5171\n",
      "Train loss 25007.5781\n",
      "Train calibration slope -0.2773\n",
      "Train accuracy: 51.2500%\n",
      "\n",
      "VALIDATION METRICS:\n",
      "Validation AUC: 0.5096\n",
      "Validation loss: 24851.0840\n",
      "Validation calibration slope: 2.0325\n",
      "Validation accuracy: 50.5051%\n",
      "==================================================\n",
      "Validation loss decreased (inf --> 24851.083984).\n",
      "Gamma: 0.023880165070295334\n",
      "Gamma 2: -0.013393701985478401\n",
      "\n",
      "CHECKPOINTED RESULTS\n",
      "Train loss: 25007.5781\n",
      "Train AUC: 51.7056%\n",
      "\n",
      "Val loss: 24851.0840\n",
      "Val AUC: 50.9592%\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "5-fold CV Accuracy on (Validation Set): 0.5170 +/- 0.0384\n",
      "5-fold CV AUC on (Validation Set): 0.5182 +/- 0.0234\n",
      "5-fold CV Calibration Slope (Validation Set): 4.8700 +/- 5.4154\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "cv_size = int(round(cv_sample_size*0.2)) # 1/5th of cv data\n",
    "holdout_size = len(test_patients) # this contains 10% of the controls + 7 cases for each of these controls\n",
    "\n",
    "start_num = 1\n",
    "for ran_search_num in range(start_num, start_num+num_of_runs):\n",
    "    print('Run name:', run_name, ran_search_num)\n",
    "    cv_indices = list(range(len(cv_input_matrices)))\n",
    "    test_indices = list(range(len(test_input_matrices)))\n",
    "    recal_test_indices = list(range(len(recal_test_input_matrices)))\n",
    "\n",
    "    # Shuffle datasets to feed randomly into the model\n",
    "    random.shuffle(cv_indices)\n",
    "    random.shuffle(test_indices)\n",
    "    random.shuffle(recal_test_indices)\n",
    "    \n",
    "    # split the cv data for cross validation\n",
    "    cv_split1_indices = cv_indices[:cv_size]\n",
    "    cv_split2_indices = cv_indices[cv_size:2*cv_size]\n",
    "    cv_split3_indices = cv_indices[2*cv_size:3*cv_size]\n",
    "    cv_split4_indices = cv_indices[3*cv_size:4*cv_size]\n",
    "    cv_split5_indices = cv_indices[4*cv_size:5*cv_size]\n",
    "\n",
    "\n",
    "    batch_size = 128 # choose a batch size\n",
    "\n",
    "    batched_graphs_split1, batched_labels_split1, batched_cv_split1_indices = utils.batch_set(indice_set=cv_split1_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split2, batched_labels_split2, batched_cv_split2_indices = utils.batch_set(indice_set=cv_split2_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split3, batched_labels_split3, batched_cv_split3_indices = utils.batch_set(indice_set=cv_split3_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split4, batched_labels_split4, batched_cv_split4_indices = utils.batch_set(indice_set=cv_split4_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split5, batched_labels_split5, batched_cv_split5_indices = utils.batch_set(indice_set=cv_split5_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    \n",
    "    \n",
    "    batched_graphs_holdout, batched_labels_holdout, batched_holdout_indices = utils.batch_set(indice_set=test_indices, input_matrices=test_input_matrices, \n",
    "                                                         labels=test_y.to_numpy(), batchsize=batch_size)\n",
    "    labels_holdout = [item for sublist in batched_labels_holdout for item in sublist]\n",
    "    \n",
    "    batched_graphs_holdout2, batched_labels_holdout2, batched_holdout2_indices = utils.batch_set(indice_set=recal_test_indices, input_matrices=recal_test_input_matrices, \n",
    "                                                         labels=recal_test_y.to_numpy(), batchsize=batch_size)\n",
    "    labels_holdout2 = [item for sublist in batched_labels_holdout2 for item in sublist]\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    # Save the true labels for analysis later\n",
    "    file_full_name_true = 'pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout1_true.npy'\n",
    "    with open(file_full_name_true, 'wb') as f:\n",
    "        np.save(f, labels_holdout)\n",
    "        \n",
    "    file_full_name_true2 = 'pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout2_true.npy'\n",
    "    with open(file_full_name_true2, 'wb') as f:\n",
    "        np.save(f, labels_holdout2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # split cv data into batches for training\n",
    "    graphs_train1, graphs_train2, graphs_train3, graphs_train4, graphs_train5, graphs_test1, graphs_test2, \\\n",
    "    graphs_test3, graphs_test4, graphs_test5, labels_train1, labels_train2, labels_train3, labels_train4, \\\n",
    "    labels_train5, labels_test1, labels_test2, labels_test3, labels_test4, labels_test5, indices_train1, \\\n",
    "    indices_train2, indices_train3, indices_train4, indices_train5, indices_test1, indices_test2, indices_test3, \\\n",
    "    indices_test4, indices_test5, \\\n",
    "    = utils.get_cv_groups(batched_graphs_split1, batched_graphs_split2, batched_graphs_split3, batched_graphs_split4, \n",
    "                    batched_graphs_split5, batched_labels_split1, batched_labels_split2, batched_labels_split3, \n",
    "                    batched_labels_split4, batched_labels_split5, batched_cv_split1_indices, batched_cv_split2_indices, \n",
    "                    batched_cv_split3_indices, batched_cv_split4_indices, batched_cv_split5_indices)\n",
    "\n",
    "    \n",
    "    nepochs=1\n",
    "    # Random hyperparameter selection\n",
    "    lr = random.choice([0.001, 0.005, 0.0001]) # initial learning rate\n",
    "\n",
    "    out_chans = random.choice([8, 16, 32]) # number of filters\n",
    "    filter_size = random.choice([3, 4, 6]) # number of time steps/ filter size\n",
    "    if no_LSTM:\n",
    "        lstm_h = 0\n",
    "    else:\n",
    "        lstm_h = random.choice([16, 32, 64, 128, 256]) # number of LSTM neurons\n",
    "    linear_size = random.choice([64, 128]) # linear layer output size\n",
    "    drop_val = random.choice([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    reg_strength = random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\n",
    "    graph_reg_strength = 1e1\n",
    "    \n",
    "\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Number of 3D CNN filters: {out_chans}\")\n",
    "    print(f\"Filter size: {filter_size}\")\n",
    "    print(f\"Number of LSTM neurons: {lstm_h}\")\n",
    "    print(f\"Number of fully connected layers: {linear_size}\")\n",
    "    print(f\"Dropout value: {drop_val}\")\n",
    "    print(f\"Regularisation strength: {reg_strength}\")\n",
    "    print(\"*\"*40)\n",
    "    \n",
    "\n",
    "    #############################################\n",
    "    ####### FOR LOOP FOR CV HERE  ###############\n",
    "    #############################################\n",
    "    \n",
    "    cv_accuracy, cv_auc, cv_cal_slope = [], [], []\n",
    "    best_metric = 0\n",
    "    for split in range(0, 5):\n",
    "        run_split_name = run_name +\"_split\" + str(split+1)\n",
    "        print(run_split_name, \"_find\")\n",
    "        \n",
    "        if demo:\n",
    "            model = whole_model_demographics.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                            exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                            fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                            no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer, num_labels=num_labels)\n",
    "        else:\n",
    "            model = whole_model.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                            exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                            fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                            no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer, num_labels=num_labels)\n",
    "    \n",
    "        early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=lr,\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        if split+1 == 1:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train1, labels_train1\n",
    "            batched_graphs_val, batched_labels_val = graphs_test1, labels_test1\n",
    "            train_indices = indices_train1\n",
    "            test_indices = indices_test1\n",
    "            \n",
    "        elif split+1 == 2:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train2, labels_train2\n",
    "            batched_graphs_val, batched_labels_val = graphs_test2, labels_test2\n",
    "            train_indices = indices_train2\n",
    "            test_indices = indices_test2\n",
    "            \n",
    "        elif split+1 == 3:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train3, labels_train3\n",
    "            batched_graphs_val, batched_labels_val = graphs_test3, labels_test3\n",
    "            train_indices = indices_train3\n",
    "            test_indices = indices_test3\n",
    "            \n",
    "        elif split+1 == 4:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train4, labels_train4\n",
    "            batched_graphs_val, batched_labels_val = graphs_test4, labels_test4\n",
    "            train_indices = indices_train4\n",
    "            test_indices = indices_test4\n",
    "            \n",
    "        elif split+1 == 5:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train5, labels_train5\n",
    "            batched_graphs_val, batched_labels_val = graphs_test5, labels_test5\n",
    "            train_indices = indices_train5\n",
    "            test_indices = indices_test5\n",
    "            \n",
    "        \n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss_epochs, train_acc_epochs, train_prec_epochs, train_recall_epochs, train_f1_epochs, train_auc_epochs = [],[],[],[],[],[]\n",
    "        train_indiv_acc_epochs, train_indiv_prec_epochs, train_indiv_recall_epochs, train_indiv_f1_epochs, train_indiv_auc_epochs = [],[],[],[],[]\n",
    "        val_loss_epochs, val_acc_epochs, val_prec_epochs, val_recall_epochs, val_f1_epochs, val_auc_epochs = [],[],[],[],[],[]\n",
    "        val_indiv_acc_epochs, val_indiv_prec_epochs, val_indiv_recall_epochs, val_indiv_f1_epochs, val_indiv_auc_epochs = [],[],[],[],[]\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{nepochs}\")\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            train_loss_list, train_acc_list, train_prec_list, train_recall_list, train_f1_list, train_auc_list = [],[],[],[],[],[]\n",
    "            train_cal_slope_list = []\n",
    "            train_all_classes_prec_list, train_all_classes_recall_list, train_all_classes_f1_list, train_all_classes_auc_list = [],[],[],[] # lists of list\n",
    "\n",
    "            for x_batch_train, y_batch_train, train_indices_list in zip(batched_graphs_trn, batched_labels_trn, train_indices):\n",
    "            # need to get one list at a time from the list of lists\n",
    "                    \n",
    "                trn_demo_vals, trn_demo_list = utils.convert_demos_to_tensor(cv_patients, train_indices_list, demo)\n",
    "                \n",
    "                if demo == False:\n",
    "                    trn_demo_vals = None\n",
    "                \n",
    "                y_batch_train=np.array(y_batch_train)\n",
    "                trn_logits, trn_loss, trn_acc, trn_prec, trn_recall, trn_auc, trn_f1, \\\n",
    "                indiv_trn_prec, indiv_trn_recall, indiv_trn_auc, indiv_trn_f1, trn_cal_slope, model,\\\n",
    "                = trnvaltst_sigmoid_oned.train_step(x_batch_train, y_batch_train, trn_demo_vals,\n",
    "                                                        reg_strength,class_weights,model,L1_ablation,L2_ablation, \n",
    "                                                       graph_reg_strength, graph_reg_incl, exponential_scaling,\n",
    "                                                       weighted_loss, variable_gamma, optimizer, demo)\n",
    "\n",
    "                train_loss_list.append(trn_loss)\n",
    "                train_cal_slope_list.append(trn_cal_slope)\n",
    "                train_acc_list.append(trn_acc)\n",
    "\n",
    "                train_all_classes_prec_list.append(trn_prec)\n",
    "                train_prec_list.append(indiv_trn_prec)\n",
    "\n",
    "                train_all_classes_recall_list.append(trn_recall)\n",
    "                train_recall_list.append(indiv_trn_recall)\n",
    "\n",
    "                train_all_classes_auc_list.append(trn_auc)\n",
    "                train_auc_list.append(indiv_trn_auc)\n",
    "\n",
    "                train_all_classes_f1_list.append(trn_f1)\n",
    "                train_f1_list.append(indiv_trn_f1)\n",
    "\n",
    "            if variable_gamma and (exponential_scaling==True):\n",
    "                gamma_val = float(model.tg_conv_layer1.gammat.numpy())\n",
    "                print(\"Gamma:\", gamma_val)\n",
    "                if second_TGCNN_layer:\n",
    "                    gamma_val2 = float(model.tg_conv_layer2.gammat.numpy())\n",
    "                    print(\"Gamma 2:\", gamma_val2)\n",
    "\n",
    "            \n",
    "\n",
    "            # get the average metric score for each class for this ONE epoch using the batch list metrics\n",
    "            train_prec_indiv_ave = utils.average_of_list_of_lists(train_all_classes_prec_list)\n",
    "            train_recall_indiv_ave = utils.average_of_list_of_lists(train_all_classes_recall_list)\n",
    "            train_f1_indiv_ave = utils.average_of_list_of_lists(train_all_classes_f1_list) # ave F1 score for each class\n",
    "            train_auc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_auc_list)\n",
    "\n",
    "            print(\"\\nTRAINING METRICS:\")\n",
    "            print(f\"Training AUC: {np.mean(train_auc_list):.4f}\")\n",
    "            print(f\"Train loss {np.mean(train_loss_list):.4f}\")\n",
    "            print(f\"Train calibration slope {np.mean(train_cal_slope_list):.4f}\")\n",
    "            print(f\"Train accuracy: {np.mean(train_acc_list) :.4%}\")\n",
    "\n",
    "\n",
    "            # Validation loop\n",
    "            val_loss_list, val_acc_list,  val_prec_list, val_recall_list, val_auc_list, val_f1_list = [],[],[],[],[],[]\n",
    "            val_cal_slope_list = []\n",
    "            val_all_classes_acc_list, val_all_classes_prec_list, val_all_classes_recall_list, val_all_classes_f1_list, val_all_classes_auc_list = [],[],[],[],[]\n",
    "            for x_batch_val, y_batch_val, test_indices_list in zip(batched_graphs_val, batched_labels_val, test_indices):\n",
    "                \n",
    "                test_demo_vals, test_demo_list = utils.convert_demos_to_tensor(cv_patients, test_indices_list, demo)                \n",
    "                \n",
    "                if demo == False:\n",
    "                    test_demo_vals = None\n",
    "                \n",
    "                val_logits, val_loss, val_acc, val_prec, val_recall, val_auc, val_f1, \\\n",
    "                indiv_val_prec, indiv_val_recall, indiv_val_auc, indiv_val_f1, val_cal_slope, model, \\\n",
    "                = trnvaltst_sigmoid_oned.val_step(x_batch_val, y_batch_val, test_demo_vals, reg_strength, \n",
    "                                                  class_weights, model, L1_ablation, L2_ablation, \n",
    "                                                  graph_reg_strength, graph_reg_incl, weighted_loss, demo)\n",
    "\n",
    "                val_loss_list.append(val_loss)\n",
    "                val_cal_slope_list.append(val_cal_slope)\n",
    "\n",
    "                val_acc_list.append(val_acc)\n",
    "\n",
    "                val_prec_list.append(val_prec)\n",
    "                val_all_classes_prec_list.append(indiv_val_prec)\n",
    "\n",
    "                val_recall_list.append(val_recall) \n",
    "                val_all_classes_recall_list.append(indiv_val_recall)\n",
    "\n",
    "                val_auc_list.append(val_auc)\n",
    "                val_all_classes_auc_list.append(indiv_val_auc)\n",
    "\n",
    "                val_f1_list.append(val_f1)\n",
    "                val_all_classes_f1_list.append(indiv_val_f1)\n",
    "            \n",
    "            #plot_figures.draw_confusion_mat(y_batch_val, val_logits, ['none','hip'], run_name=None, ran_search_num=1111, data_type=\"V\")\n",
    "            #plot_figures.draw_calibration_curve(y_batch_val, val_logits, run_name=None, ran_search_num=1111)\n",
    "            \n",
    "            \n",
    "            val_prec_indiv_ave = utils.average_of_list_of_lists(val_all_classes_prec_list)\n",
    "            val_recall_indiv_ave = utils.average_of_list_of_lists(val_all_classes_recall_list)\n",
    "            val_f1_indiv_ave = utils.average_of_list_of_lists(val_all_classes_f1_list) # ave F1 score for each class\n",
    "            val_auc_indiv_ave = utils.average_of_list_of_lists(val_all_classes_auc_list)\n",
    "            \n",
    "            # Saves the logits if the AUC improves\n",
    "            improved_metric = False\n",
    "            if np.mean(val_auc_list) > best_metric:\n",
    "                best_metric = np.mean(val_auc_list)\n",
    "                improved_metric = True\n",
    "\n",
    "            print(\"\\nVALIDATION METRICS:\")\n",
    "            print(f\"Validation AUC: {np.mean(val_auc_list):.4f}\")\n",
    "            print(f\"Validation loss: {np.mean(val_loss_list):.4f}\")\n",
    "            print(f\"Validation calibration slope: {np.mean(val_cal_slope_list):.4f}\")\n",
    "            print(f\"Validation accuracy: {np.mean(val_acc_list) :.4%}\")\n",
    "\n",
    "\n",
    "\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # get the average metric from ONE epoch\n",
    "            ave_epoch_train_loss, ave_epoch_train_acc, ave_epoch_train_auc, ave_epoch_train_prec, ave_epoch_train_recall, ave_epoch_train_f1 = np.mean(train_loss_list), np.mean(train_acc_list), np.mean(train_auc_list), np.mean(train_prec_list), np.mean(train_recall_list), np.mean(train_f1_list)\n",
    "            ave_epoch_val_loss, ave_epoch_val_acc, ave_epoch_val_auc, ave_epoch_val_prec, ave_epoch_val_recall, ave_epoch_val_f1 = np.mean(val_loss_list), np.mean(val_acc_list), np.mean(val_auc_list), np.mean(val_prec_list), np.mean(val_recall_list), np.mean(val_f1_list)        \n",
    "\n",
    "\n",
    "            #print(optimizer.get_config())\n",
    "\n",
    "            # save the average (from one epoch) to the list of ALL epochs\n",
    "            utils.metric_save(ave_epoch_train_loss, train_loss_epochs, ave_epoch_val_loss, val_loss_epochs) #losses\n",
    "            utils.metric_save(ave_epoch_train_acc, train_acc_epochs, ave_epoch_val_acc, val_acc_epochs) # acc\n",
    "            utils.metric_save(ave_epoch_train_auc, train_auc_epochs, ave_epoch_val_auc, val_auc_epochs) # auc        \n",
    "            utils.metric_save(ave_epoch_train_prec, train_prec_epochs, ave_epoch_val_prec, val_prec_epochs) # prec\n",
    "            utils.metric_save(ave_epoch_train_recall, train_recall_epochs, ave_epoch_val_recall, val_recall_epochs) # recall\n",
    "            utils.metric_save(ave_epoch_train_f1, train_f1_epochs, ave_epoch_val_f1, val_f1_epochs) #f1\n",
    "\n",
    "            # save the average from one epoch to the list of ALL epochs for each individual class\n",
    "            utils.metric_save(train_auc_indiv_ave, train_indiv_auc_epochs, val_auc_indiv_ave, val_indiv_auc_epochs)\n",
    "            utils.metric_save(train_prec_indiv_ave, train_indiv_prec_epochs, val_prec_indiv_ave, val_indiv_prec_epochs)\n",
    "            utils.metric_save(train_recall_indiv_ave, train_indiv_recall_epochs, val_recall_indiv_ave, val_indiv_recall_epochs)\n",
    "            utils.metric_save(train_f1_indiv_ave, train_indiv_f1_epochs, val_f1_indiv_ave, val_indiv_f1_epochs)\n",
    "\n",
    "\n",
    "            early_stopping_metric = val_loss_list # metric that is used to determine if the model should stop training\n",
    "            early_stopping(np.mean(early_stopping_metric), train_loss_list, val_loss_list, \n",
    "                           train_cal_slope_list, val_cal_slope_list,\n",
    "                           train_acc_list, val_acc_list,  \n",
    "                           train_auc_list, val_auc_list,\n",
    "                           train_auc_indiv_ave, val_auc_indiv_ave,\n",
    "                           train_prec_list, val_prec_list,\n",
    "                           train_prec_indiv_ave, val_prec_indiv_ave,\n",
    "                           train_recall_list, val_recall_list,\n",
    "                           train_recall_indiv_ave, val_recall_indiv_ave,\n",
    "                           train_f1_list, val_f1_list,\n",
    "                           train_f1_indiv_ave, val_f1_indiv_ave, \n",
    "                           model, run_name+str(ran_search_num),\n",
    "                           batched_graphs_holdout, batched_labels_holdout, batched_holdout_indices,\n",
    "                           batched_graphs_holdout2, batched_labels_holdout2, batched_holdout2_indices,\n",
    "                           reg_strength,\n",
    "                           class_weights, L1_ablation, L2_ablation, graph_reg_strength, graph_reg_incl, weighted_loss, \n",
    "                           improved_metric, test_patients, recal_test_patients, \n",
    "                           demo)\n",
    "\n",
    "            if early_stopping.checkpoint_made:\n",
    "\n",
    "                checkpoint_train_loss, checkpoint_val_loss, \\\n",
    "                checkpoint_train_cal_slope, checkpoint_val_cal_slope, \\\n",
    "                checkpoint_train_acc, checkpoint_val_acc, \\\n",
    "                checkpoint_train_auc, checkpoint_val_auc, \\\n",
    "                checkpoint_train_auc_indiv, checkpoint_val_auc_indiv, \\\n",
    "                checkpoint_train_prec, checkpoint_val_prec,  \\\n",
    "                checkpoint_train_prec_indiv, checkpoint_val_prec_indiv, \\\n",
    "                checkpoint_train_recall, checkpoint_val_recall, \\\n",
    "                checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, \\\n",
    "                checkpoint_train_f1, checkpoint_val_f1, \\\n",
    "                checkpoint_train_f1_indiv, checkpoint_val_f1_indiv = early_stopping.print_checkpoint_metric(train_loss_list, \n",
    "                                                                            val_loss_list, \n",
    "                                                                            train_cal_slope_list, val_cal_slope_list,\n",
    "                                                                            train_acc_list, val_acc_list, \n",
    "                                                                            train_auc_list, val_auc_list, \n",
    "                                                                            train_auc_indiv_ave, val_auc_indiv_ave,\n",
    "                                                                            train_prec_list, val_prec_list,\n",
    "                                                                            train_prec_indiv_ave, val_prec_indiv_ave,                                        \n",
    "                                                                            train_recall_list, val_recall_list,\n",
    "                                                                            train_recall_indiv_ave, val_recall_indiv_ave,                                        \n",
    "                                                                            train_f1_list, val_f1_list,\n",
    "                                                                            train_f1_indiv_ave, val_f1_indiv_ave)\n",
    "\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        if variable_gamma and (exponential_scaling==True):\n",
    "            gamma_val = float(model.tg_conv_layer1.gammat.numpy())\n",
    "            print(\"Gamma:\", gamma_val)\n",
    "            if second_TGCNN_layer:\n",
    "                gamma_val2 = float(model.tg_conv_layer2.gammat.numpy())\n",
    "                print(\"Gamma 2:\", gamma_val2)\n",
    "                \n",
    "        else:\n",
    "            gamma_val = 'N/A'\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        time_taken = timedelta(seconds=end_time - start_time)\n",
    "\n",
    "        list_for_csv = [run_split_name+\"_\"+str(ran_search_num), nepochs, epoch+1, str(time_taken), lr, out_chans, filter_size, \n",
    "                        lstm_h, reg_strength,\n",
    "                       linear_size, drop_val, batch_size,\n",
    "\n",
    "                        min(train_loss_epochs), max(train_loss_epochs), min(val_loss_epochs), \n",
    "                        max(val_loss_epochs), checkpoint_train_loss, checkpoint_val_loss,                    \n",
    "\n",
    "                        min(train_auc_epochs), max(train_auc_epochs), min(val_auc_epochs), \n",
    "                        max(val_auc_epochs), checkpoint_train_auc, checkpoint_val_auc,\n",
    "                        checkpoint_train_auc_indiv, checkpoint_val_auc_indiv,                    \n",
    "\n",
    "                        min(train_acc_epochs), max(train_acc_epochs), min(val_acc_epochs), \n",
    "                        max(val_acc_epochs), checkpoint_train_acc, checkpoint_val_acc,\n",
    "\n",
    "                        min(train_prec_epochs), max(train_prec_epochs), min(val_prec_epochs), \n",
    "                        max(val_prec_epochs), checkpoint_train_prec, checkpoint_val_prec,   \n",
    "                        checkpoint_train_prec_indiv, checkpoint_val_prec_indiv,\n",
    "\n",
    "                        min(train_recall_epochs), max(train_recall_epochs), min(val_recall_epochs), \n",
    "                        max(val_recall_epochs), checkpoint_train_recall, checkpoint_val_recall, \n",
    "                        checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, \n",
    "\n",
    "                        min(train_f1_epochs), max(train_f1_epochs), min(val_f1_epochs), \n",
    "                        max(val_f1_epochs), checkpoint_train_f1, checkpoint_val_f1, \n",
    "                        checkpoint_train_f1_indiv, checkpoint_val_f1_indiv,\n",
    "\n",
    "                        gamma_val, \n",
    "                        checkpoint_train_cal_slope, checkpoint_val_cal_slope,\n",
    "                        activation_type, LSTM_str, exp_str, timestamp_str, weighted_loss_str, L1_str, L2_str,\n",
    "                        second_layer_str]\n",
    "\n",
    "\n",
    "\n",
    "        with open('results/cross_validation_results.csv', 'a', newline='') as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(list_for_csv)\n",
    "            f_object.close()\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\nCHECKPOINTED RESULTS\")\n",
    "        print(f\"Train loss: {checkpoint_train_loss:.4f}\") # average loss from the epoch\n",
    "        print(f\"Train AUC: {checkpoint_train_auc:.4%}\\n\")\n",
    "        print(f\"Val loss: {checkpoint_val_loss:.4f}\") # average loss from the epoch\n",
    "        print(f\"Val AUC: {checkpoint_val_auc:.4%}\\n\")\n",
    "\n",
    "        print(\"=\"*50)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        cv_accuracy.append(checkpoint_val_acc)\n",
    "        cv_auc.append(checkpoint_val_auc)\n",
    "        cv_cal_slope.append(checkpoint_val_cal_slope)\n",
    "        \n",
    "    print(f\"5-fold CV Accuracy on (Validation Set): {np.mean(cv_accuracy):.4f} +/- {np.std(cv_accuracy):.4f}\")\n",
    "    print(f\"5-fold CV AUC on (Validation Set): {np.mean(cv_auc):.4f} +/- {np.std(cv_auc):.4f}\")\n",
    "    print(f\"5-fold CV Calibration Slope (Validation Set): {np.mean(cv_cal_slope):.4f} +/- {np.std(cv_cal_slope):.4f}\")\n",
    "    print(\"*\"*100)\n",
    "        \n",
    "    class_names = ['none', 'hip']\n",
    "#     plot_figures.plot_loss_curve(train_loss = train_loss_epochs, val_loss = val_loss_epochs, test_lost = test_loss_epochs, \n",
    "#                     run_name=run_split_name, ran_search_num=ran_search_num)\n",
    "\n",
    "\n",
    "    # open the proba file and then plot the figures for the test data\n",
    "    # The model is tested during early stopping\n",
    "    with open('pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout1_proba.npy', 'rb') as f:\n",
    "        test_logits = np.load(f)\n",
    "\n",
    "    #plot_figures.draw_confusion_mat(labels_holdout, test_logits, class_names, run_name, ran_search_num)\n",
    "    #plot_figures.draw_calibration_curve(labels_holdout, test_logits, run_name, ran_search_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20534f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02937296, -0.02202623,  0.04310495, ...,  0.01448617,\n",
       "         0.03389565,  0.02068298],\n",
       "       [-0.01197459,  0.02108628,  0.0312911 , ...,  0.04077946,\n",
       "         0.04228025,  0.01532209],\n",
       "       [-0.0044372 ,  0.06639115, -0.00612057, ...,  0.02373674,\n",
       "         0.08374877, -0.05779389],\n",
       "       ...,\n",
       "       [ 0.05281276,  0.1276532 ,  0.02078438, ..., -0.04240575,\n",
       "        -0.01686185, -0.00970305],\n",
       "       [-0.07748396,  0.07232463, -0.04917607, ..., -0.06415091,\n",
       "         0.08160617, -0.04094023],\n",
       "       [-0.03634715, -0.04259878,  0.02829995, ..., -0.0060974 ,\n",
       "        -0.07509752,  0.03613949]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tg_conv_layer1.trainable_weights[0].numpy()\n",
    "with open('../model_explainability/cnn_filters/'+run_name+'_filter.npy', 'wb') as f:\n",
    "    np.save(f, model.tg_conv_layer1.trainable_weights[0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff48882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "483.316px",
    "left": "1447.26px",
    "right": "20px",
    "top": "158.896px",
    "width": "529.462px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
