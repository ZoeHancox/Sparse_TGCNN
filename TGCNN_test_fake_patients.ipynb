{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609f696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.694837Z",
     "start_time": "2022-12-20T15:00:03.051041Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from src import utils, trnvaltst, TGCNN_layer, whole_model, create_fake_patients\n",
    "from early_stopping import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, roc_auc_score, recall_score\n",
    "from csv import writer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"tensorflow version:\", tf. __version__)\n",
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f64909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.709798Z",
     "start_time": "2022-12-20T15:00:24.696832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_LSTM = False # True = without LSTM\n",
    "no_exponential = True # True = without exponential\n",
    "L1_ablation = True # True = with L1 reg\n",
    "L2_ablation = True # True = with L2 reg\n",
    "variable_gamma = False\n",
    "graph_reg_incl = False\n",
    "\n",
    "num_of_runs = 1\n",
    "\n",
    "if no_exponential == True:\n",
    "    exponential_scaling = False\n",
    "else:\n",
    "    exponential_scaling = True    \n",
    "    \n",
    "weighted_loss = False # class weighted to deal with imbalance if True\n",
    "no_timestamp = False # if no_timestamp = True then all values in 3-tensor = 1\n",
    "activation_type = 'gelu' #'relu','gelu', 'LeakyReLU'\n",
    "second_TGCNN_layer = True\n",
    "\n",
    "run_name='test_model' + activation_type \n",
    "\n",
    "# strings for hyperparameter searching file\n",
    "LSTM_str=\"LSTM excluded\" if no_LSTM == True else \"LSTM included\"\n",
    "exp_str = \"exp excluded\" if no_exponential == False else \"exp included\"\n",
    "timestamp_str = \"time elapsed = 1\" if no_timestamp == True else \"time elapsed\"\n",
    "weighted_loss_str = \"weighted_loss\" if weighted_loss ==True else \"unweighted_loss\"\n",
    "L1_str = \"L1 included\" if L1_ablation == True else \"L1 excluded\"\n",
    "L2_str = \"L2 included\" if L2_ablation == True else \"L2 excluded\"\n",
    "second_layer_str = \"Branched model\" if second_TGCNN_layer == True else \"Unbranched model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f3756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:02:15.671159Z",
     "start_time": "2022-12-20T15:00:24.710799Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_codes_to_use = 512\n",
    "max_event_codes = 512\n",
    "max_timesteps = 100\n",
    "num_patients = 50000\n",
    "\n",
    "# read in a pd df or pickle below\n",
    "input_values_indices_df = create_fake_patients.create_fake_patient_df(num_patients=num_patients+1, max_events=200, max_nodes=10)\n",
    "\n",
    "sample_size = len(input_values_indices_df)\n",
    "\n",
    "# y, num_in_cat_df = utils.load_in_y_visit_count(num_codes_to_use)\n",
    "# num_in_cat_dict = num_in_cat_df.to_dict()\n",
    "\n",
    "# create dictionary of 'labels'\n",
    "y = create_fake_patients.create_fake_int_label(num_patients)\n",
    "\n",
    "# count the number of occurrences of each value\n",
    "num_in_cat_df = value_counts = y['int_label'].value_counts()\n",
    "# create a dictionary to map the values to labels\n",
    "value_dict = {0: 'Zero', 1: 'Low', 2: 'High'}\n",
    "\n",
    "# create a new DataFrame with the value counts\n",
    "df_value_counts = pd.DataFrame({'Value': value_counts.index, 'Count': value_counts.values})\n",
    "\n",
    "# replace the values in the \"Value\" column with the corresponding labels\n",
    "df_value_counts['Value'] = df_value_counts['Value'].map(value_dict)\n",
    "\n",
    "# convert the DataFrame to a dictionary\n",
    "num_in_cat_dict = df_value_counts.set_index('Value')['Count'].to_dict()\n",
    "\n",
    "# some people will be removed due to having 5 or less historic data records (=<5 visits). \n",
    "# Also any people that whom all of their Read codes do not appear in the top 1000 codes\n",
    "\n",
    "\n",
    "print(f\"Number of people in the input data {len(input_values_indices_df)}\")\n",
    "print(f\"Number of people in label data: {y.shape[0]}\")\n",
    "y = y[:sample_size+1]\n",
    "print(f\"Number of people in label data: {y.shape[0]}\")\n",
    "low_norm, high_norm, zero_norm = utils.normalised_inv_class_proportion(num_in_cat_dict)\n",
    "class_weights = tf.compat.v2.constant([[high_norm, low_norm, zero_norm]]) # this needs to be in the order of class occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c8eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.080921Z",
     "start_time": "2022-12-20T15:02:15.672664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_matrices = []\n",
    "\n",
    "\n",
    "for patient in range(sample_size):\n",
    "#for patient in range(10240):\n",
    "    \n",
    "    i_list = input_values_indices_df.iloc[patient]['indices'] # indices from patient cell\n",
    "    v_list = input_values_indices_df.iloc[patient]['values'] # values from patient cell\n",
    "    #print(i_list)\n",
    "    \n",
    "    individual_sparse = tf.sparse.SparseTensor(i_list, v_list, (max_event_codes, max_event_codes, max_timesteps))\n",
    "    #print(individual_sparse)\n",
    "    \n",
    "    # adding the sparse tensor to a list of all the tensors\n",
    "    ordered_indiv = tf.sparse.reorder(individual_sparse) # reorder required for tensor to work (no effect to outcome)\n",
    "    input_matrices.append(tf.sparse.expand_dims(ordered_indiv, axis=0))\n",
    "    \n",
    "    if patient%10000 == 0:\n",
    "        print(f\"{patient}/{sample_size} converted to SparseTensors {patient/sample_size:.2%}\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e70c1d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb920bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.095923Z",
     "start_time": "2022-12-20T15:04:46.082885Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for cross validation the data could be split into 5 folds each which make up 20% of the data\n",
    "# but for now just split into train, validation and test\n",
    "train_perc = 0.7\n",
    "val_perc = 0.15\n",
    "test_perc = 0.15\n",
    "\n",
    "train_size = int(round(sample_size*train_perc))\n",
    "val_size = int(round(sample_size*val_perc))\n",
    "test_size = int(round(sample_size*test_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bb951",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6e878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.111881Z",
     "start_time": "2022-12-20T15:04:46.096918Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the average epoch to the list of all epochs\n",
    "def metric_save(trn_epoch_metric, trn_all_epoch_avgs, \n",
    "            val_epoch_metric, val_all_epoch_avgs,\n",
    "            test_epoch_metric, test_all_epoch_avgs):\n",
    "    trn_all_epoch_avgs.append(trn_epoch_metric)\n",
    "    val_all_epoch_avgs.append(val_epoch_metric)\n",
    "    test_all_epoch_avgs.append(test_epoch_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72877dff",
   "metadata": {},
   "source": [
    "# Loop for hyperparameter tuning starts here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0756d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Repeated Hyperparameter Selection - not random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b4c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:04:46.127968Z",
     "start_time": "2022-12-20T15:04:46.112875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    ############################# Repeated Hyperparameter Selection #############################\n",
    "#     nepochs = 20\n",
    "#     lr = 1e-2 #initial learning rate\n",
    "#     out_chans = 1 # number of filters\n",
    "#     filter_size = 3 # number of time steps/ filter size\n",
    "#     if no_LSTM:\n",
    "#         lstm_h = 0\n",
    "#     else:\n",
    "#         lstm_h = 128 # number of LSTM neurons\n",
    "    \n",
    "#     reg_strength = 1e-1#1e3\n",
    "#     linear_size = 512 # linear layer output size\n",
    "#     drop_val = 0.5\n",
    "    ############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b99e2",
   "metadata": {},
   "source": [
    "### Random Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14366ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.699086Z",
     "start_time": "2022-12-20T15:04:46.128962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ran_search_num in range(num_of_runs):\n",
    "    #random.seed(time.time())\n",
    "    print('Run name:', run_name, ran_search_num)\n",
    "    indices = list(range(len(input_matrices)))\n",
    "    random.shuffle(indices) # shuffles datasets\n",
    "    train_set_indices = indices[:train_size]\n",
    "    val_set_indices = indices[train_size:train_size+val_size]\n",
    "    test_set_indices = indices[-test_size:]\n",
    "\n",
    "    train_batch_size = 1024\n",
    "    val_batch_size = 1024\n",
    "    test_batch_size = 1024\n",
    "    \n",
    "    batched_graphs_trn, batched_labels_trn = utils.batch_set(indice_set=train_set_indices, input_matrices=input_matrices, \n",
    "                                                       labels=y, batchsize=train_batch_size)\n",
    "    batched_graphs_val, batched_labels_val = utils.batch_set(indice_set=val_set_indices, input_matrices=input_matrices, \n",
    "                                                       labels=y, batchsize=val_batch_size)\n",
    "    batched_graphs_test, batched_labels_test = utils.batch_set(indice_set=test_set_indices, input_matrices=input_matrices, \n",
    "                                                         labels=y, batchsize=test_batch_size)\n",
    "    \n",
    "\n",
    "    #nepochs = random.choice([25, 50, 75, 100])\n",
    "    nepochs=200\n",
    "    lr = random.choice([0.01, 0.05, 0.001, 0.005, 0.0001]) # initial learning rate\n",
    "\n",
    "    out_chans = random.choice([8, 16, 32]) # number of filters\n",
    "    filter_size = random.choice([3, 4, 6]) # number of time steps/ filter size\n",
    "    if no_LSTM:\n",
    "        lstm_h = 0\n",
    "    else:\n",
    "        lstm_h = random.choice([16, 32, 64, 128, 256]) # number of LSTM neurons\n",
    "    linear_size = random.choice([64, 128]) # linear layer output size\n",
    "    #drop_val = random.choice([0.2, 0.3, 0.4, 0.5])\n",
    "    drop_val = random.choice([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    reg_strength = random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\n",
    "    #reg_strength = random.choice([100, 10, 1, 1e-1, 5e-1])\n",
    "    graph_reg_strength = 1e1\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Number of 3D CNN filters: {out_chans}\")\n",
    "    print(f\"Filter size: {filter_size}\")\n",
    "    print(f\"Number of LSTM neurons: {lstm_h}\")\n",
    "    print(f\"Number of fully connected layers: {linear_size}\")\n",
    "    print(f\"Dropout value: {drop_val}\")\n",
    "    print(f\"Regularisation strength: {reg_strength}\")\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(\"*\"*40)\n",
    "    model = whole_model.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                        filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                        exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                       fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                        no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    #cce_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    epochs = nepochs\n",
    "    train_loss_epochs, train_acc_epochs, train_prec_epochs, train_recall_epochs, train_f1_epochs, train_auc_epochs = [],[],[],[],[],[]\n",
    "    train_indiv_acc_epochs, train_indiv_prec_epochs, train_indiv_recall_epochs, train_indiv_f1_epochs, train_indiv_auc_epochs = [],[],[],[],[]\n",
    "    val_loss_epochs, val_acc_epochs, val_prec_epochs, val_recall_epochs, val_f1_epochs, val_auc_epochs = [],[],[],[],[],[]\n",
    "    val_indiv_acc_epochs, val_indiv_prec_epochs, val_indiv_recall_epochs, val_indiv_f1_epochs, val_indiv_auc_epochs = [],[],[],[],[]\n",
    "    test_loss_epochs, test_acc_epochs, test_prec_epochs, test_recall_epochs, test_f1_epochs, test_auc_epochs = [],[],[],[],[],[]\n",
    "    test_indiv_acc_epochs, test_indiv_prec_epochs, test_indiv_recall_epochs, test_indiv_f1_epochs, test_indiv_auc_epochs = [],[],[],[],[]\n",
    "    for epoch in range(epochs):\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}/{nepochs}\")\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        train_loss_list, train_acc_list, train_prec_list, train_recall_list, train_f1_list, train_auc_list = [],[],[],[],[],[]\n",
    "        #train_all_classes_acc_list, \n",
    "        train_all_classes_prec_list, train_all_classes_recall_list, train_all_classes_f1_list, train_all_classes_auc_list = [],[],[],[] # lists of lists\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(zip(batched_graphs_trn, batched_labels_trn)):\n",
    "            trn_logits, trn_loss, trn_acc, trn_prec, trn_recall, trn_auc, trn_f1, \\\n",
    "            indiv_trn_prec, indiv_trn_recall, indiv_trn_auc, indiv_trn_f1, \\\n",
    "            = trnvaltst.train_step(x_batch_train,y_batch_train,reg_strength,class_weights,model,L1_ablation,L2_ablation,\n",
    "                         weighted_loss, variable_gamma, optimizer)\n",
    "            \n",
    "            train_loss_list.append(trn_loss)\n",
    "            train_acc_list.append(trn_acc)\n",
    "        \n",
    "            train_all_classes_prec_list.append(trn_prec)\n",
    "            train_prec_list.append(indiv_trn_prec)\n",
    "\n",
    "            train_all_classes_recall_list.append(trn_recall)\n",
    "            train_recall_list.append(indiv_trn_recall)\n",
    "            \n",
    "            train_all_classes_auc_list.append(trn_auc)\n",
    "            train_auc_list.append(indiv_trn_auc)\n",
    "            \n",
    "            train_all_classes_f1_list.append(trn_f1)\n",
    "            train_f1_list.append(indiv_trn_f1)\n",
    "\n",
    "        \n",
    "        # get the average metric score for each class for this ONE epoch using the batch list metrics\n",
    "        #train_acc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_acc_list) # average accuracy score for each class over one epoch\n",
    "        train_prec_indiv_ave = utils.average_of_list_of_lists(train_all_classes_prec_list)\n",
    "        train_recall_indiv_ave = utils.average_of_list_of_lists(train_all_classes_recall_list)\n",
    "        train_f1_indiv_ave = utils.average_of_list_of_lists(train_all_classes_f1_list) # ave F1 score for each class\n",
    "        train_auc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_auc_list)\n",
    "        \n",
    "        print(\"\\nTRAINING METRICS:\")\n",
    "#         print(f\"Training individual precision scores: {train_prec_indiv_ave}\")\n",
    "#         print(f\"Training individual recall scores: {train_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Training individual F1 scores: {train_f1_indiv_ave}\")\n",
    "#         print(f\"Training individual AUC scores: {train_auc_indiv_ave}\")\n",
    "        print(f\"Train loss {np.mean(train_loss_list):.4f}\")\n",
    "        print(f\"Train accuracy: {np.mean(train_acc_list) :.4%}\")\n",
    "        \n",
    " \n",
    "    \n",
    "\n",
    "        # Validation loop\n",
    "        val_loss_list, val_acc_list,  val_prec_list, val_recall_list, val_auc_list, val_f1_list = [],[],[],[],[],[]\n",
    "        val_all_classes_acc_list, val_all_classes_prec_list, val_all_classes_recall_list, val_all_classes_f1_list, val_all_classes_auc_list = [],[],[],[],[]\n",
    "        for x_batch_val, y_batch_val in zip(batched_graphs_val, batched_labels_val):\n",
    "            val_logits, val_loss, val_acc, val_prec, val_recall, val_auc, val_f1, \\\n",
    "            indiv_val_prec, indiv_val_recall, indiv_val_auc, indiv_val_f1, \\\n",
    "            = trnvaltst.val_step(x_batch_val,y_batch_val,reg_strength,class_weights,model,L1_ablation,weighted_loss)\n",
    "\n",
    "            val_loss_list.append(val_loss)\n",
    "            \n",
    "            val_acc_list.append(val_acc)\n",
    "            \n",
    "            val_prec_list.append(val_prec)\n",
    "            val_all_classes_prec_list.append(indiv_val_prec)\n",
    "            \n",
    "            val_recall_list.append(val_recall) \n",
    "            val_all_classes_recall_list.append(indiv_val_recall)\n",
    "            \n",
    "            val_auc_list.append(val_auc)\n",
    "            val_all_classes_auc_list.append(indiv_val_auc)\n",
    "            \n",
    "            val_f1_list.append(val_f1)\n",
    "            val_all_classes_f1_list.append(indiv_val_f1)\n",
    "            \n",
    "        val_prec_indiv_ave = utils.average_of_list_of_lists(val_all_classes_prec_list)\n",
    "        val_recall_indiv_ave = utils.average_of_list_of_lists(val_all_classes_recall_list)\n",
    "        val_f1_indiv_ave = utils.average_of_list_of_lists(val_all_classes_f1_list) # ave F1 score for each class\n",
    "        val_auc_indiv_ave = utils.average_of_list_of_lists(val_all_classes_auc_list)\n",
    "\n",
    "        print(\"\\nVALIDATION METRICS:\")\n",
    "#         print(f\"Validation individual precision scores: {val_prec_indiv_ave}\")\n",
    "#         print(f\"Validation individual recall scores: {val_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Validation individual F1 scores: {val_f1_indiv_ave}\")\n",
    "#         print(f\"Validation individual AUC scores: {val_auc_indiv_ave}\")\n",
    "        print(f\"Validation loss: {np.mean(val_loss_list):.4f}\")\n",
    "        print(f\"Validation accuracy: {np.mean(val_acc_list) :.4%}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Test loop\n",
    "        test_loss_list, test_acc_list, test_prec_list, test_recall_list, test_auc_list, test_f1_list = [],[],[],[],[],[]\n",
    "        train_all_classes_acc_list, test_all_classes_prec_list, test_all_classes_recall_list, test_all_classes_f1_list, test_all_classes_auc_list = [],[],[],[],[]\n",
    "        for x_batch_test, y_batch_test in zip(batched_graphs_test, batched_labels_test):\n",
    "            test_logits, test_loss, test_acc, test_prec, test_recall, test_auc, test_f1, \\\n",
    "            indiv_test_prec, indiv_test_recall, indiv_test_auc, indiv_test_f1, \\\n",
    "            = trnvaltst.val_step(x_batch_test,y_batch_test,reg_strength,class_weights,model,L1_ablation,weighted_loss)\n",
    "\n",
    "            test_loss_list.append(test_loss)\n",
    "            test_acc_list.append(test_acc)\n",
    "            test_prec_list.append(test_prec)\n",
    "            test_all_classes_prec_list.append(indiv_test_prec)\n",
    "            \n",
    "            test_recall_list.append(test_recall) \n",
    "            test_all_classes_recall_list.append(indiv_test_recall)\n",
    "            \n",
    "            test_auc_list.append(test_auc)\n",
    "            test_all_classes_auc_list.append(indiv_test_auc)\n",
    "            \n",
    "            test_f1_list.append(test_f1)\n",
    "            test_all_classes_f1_list.append(indiv_test_f1)\n",
    "            \n",
    "            \n",
    "        test_prec_indiv_ave = utils.average_of_list_of_lists(test_all_classes_prec_list)\n",
    "        test_recall_indiv_ave = utils.average_of_list_of_lists(test_all_classes_recall_list)\n",
    "        test_f1_indiv_ave = utils.average_of_list_of_lists(test_all_classes_f1_list) # ave F1 score for each class\n",
    "        test_auc_indiv_ave = utils.average_of_list_of_lists(test_all_classes_auc_list)        \n",
    "        print(\"\\nTEST METRICS:\")\n",
    "#         print(f\"Test individual precision scores: {test_prec_indiv_ave}\")\n",
    "#         print(f\"Test individual recall scores: {test_recall_indiv_ave}\") # true positive rate\n",
    "#         print(f\"Test individual F1 scores: {test_f1_indiv_ave}\")\n",
    "#         print(f\"Test individual AUC scores: {test_auc_indiv_ave}\")\n",
    "        print(f\"Test loss: {np.mean(test_loss_list):.4f}\") # average loss from the epoch\n",
    "        print(f\"Test accuracy: {np.mean(test_acc_list) :.4%}\")\n",
    "#         print(f\"Test precision: {np.mean(test_prec_list) :.4f}\")\n",
    "#         print(f\"Test recall: {np.mean(test_recall_list) :.4f}\")\n",
    "\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # get the average metric from ONE epoch\n",
    "        ave_epoch_train_loss, ave_epoch_train_acc, ave_epoch_train_auc, ave_epoch_train_prec, ave_epoch_train_recall, ave_epoch_train_f1 = np.mean(train_loss_list), np.mean(train_acc_list), np.mean(train_auc_list), np.mean(train_prec_list), np.mean(train_recall_list), np.mean(train_f1_list)\n",
    "        ave_epoch_val_loss, ave_epoch_val_acc, ave_epoch_val_auc, ave_epoch_val_prec, ave_epoch_val_recall, ave_epoch_val_f1 = np.mean(val_loss_list), np.mean(val_acc_list), np.mean(val_auc_list), np.mean(val_prec_list), np.mean(val_recall_list), np.mean(val_f1_list)\n",
    "        ave_epoch_test_loss, ave_epoch_test_acc, ave_epoch_test_auc, ave_epoch_test_prec, ave_epoch_test_recall, ave_epoch_test_f1 = np.mean(test_loss_list), np.mean(test_acc_list), np.mean(val_auc_list), np.mean(test_prec_list), np.mean(test_recall_list), np.mean(test_f1_list)\n",
    "\n",
    "\n",
    "        #print(optimizer.get_config())\n",
    "\n",
    "        # save the average (from one epoch) to the list of ALL epochs\n",
    "        metric_save(ave_epoch_train_loss, train_loss_epochs, ave_epoch_val_loss, val_loss_epochs, \n",
    "                    ave_epoch_test_loss, test_loss_epochs) #losses\n",
    "        metric_save(ave_epoch_train_acc, train_acc_epochs, ave_epoch_val_acc, val_acc_epochs, \n",
    "                    ave_epoch_test_acc, test_acc_epochs) # acc\n",
    "        metric_save(ave_epoch_train_auc, train_auc_epochs, ave_epoch_val_auc, val_auc_epochs, \n",
    "                    ave_epoch_test_auc, test_auc_epochs) # auc        \n",
    "        metric_save(ave_epoch_train_prec, train_prec_epochs, ave_epoch_val_prec, val_prec_epochs, \n",
    "                    ave_epoch_test_prec, test_prec_epochs) # prec\n",
    "        metric_save(ave_epoch_train_recall, train_recall_epochs, ave_epoch_val_recall, \n",
    "                    val_recall_epochs, ave_epoch_test_recall, test_recall_epochs) # recall\n",
    "        metric_save(ave_epoch_train_f1, train_f1_epochs, ave_epoch_val_f1, \n",
    "                    val_f1_epochs, ave_epoch_test_f1, test_f1_epochs)\n",
    "        \n",
    "        ################## save the average from one epoch to the list of ALL epochs for each individual class\n",
    "        metric_save(train_auc_indiv_ave, train_indiv_auc_epochs, val_auc_indiv_ave, val_indiv_auc_epochs,\n",
    "                   test_auc_indiv_ave, test_indiv_auc_epochs)\n",
    "        metric_save(train_prec_indiv_ave, train_indiv_prec_epochs, val_prec_indiv_ave, val_indiv_prec_epochs,\n",
    "                   test_prec_indiv_ave, test_indiv_prec_epochs)\n",
    "        metric_save(train_recall_indiv_ave, train_indiv_recall_epochs, val_recall_indiv_ave, val_indiv_recall_epochs,\n",
    "                   test_recall_indiv_ave, test_indiv_recall_epochs)\n",
    "        metric_save(train_f1_indiv_ave, train_indiv_f1_epochs, val_f1_indiv_ave, val_indiv_f1_epochs,\n",
    "                   test_f1_indiv_ave, test_indiv_f1_epochs)\n",
    "\n",
    "        \n",
    "        early_stopping_metric = val_loss_list # metric that is used to determine if the model should stop training\n",
    "        early_stopping(np.mean(early_stopping_metric), train_loss_list, val_loss_list, \n",
    "                       test_loss_list, train_acc_list, val_acc_list, \n",
    "                       test_acc_list, \n",
    "                       train_auc_list, val_auc_list, test_auc_list,\n",
    "                       train_auc_indiv_ave, val_auc_indiv_ave, test_auc_indiv_ave,\n",
    "                       \n",
    "                       train_prec_list, val_prec_list, test_prec_list,\n",
    "                       train_prec_indiv_ave, val_prec_indiv_ave, test_prec_indiv_ave,\n",
    "                       \n",
    "                       train_recall_list, val_recall_list, test_recall_list,\n",
    "                       train_recall_indiv_ave, val_recall_indiv_ave, test_recall_indiv_ave,\n",
    "                       \n",
    "                       train_f1_list, val_f1_list, test_f1_list,\n",
    "                       train_f1_indiv_ave, val_f1_indiv_ave, test_f1_indiv_ave\n",
    "                      )\n",
    "\n",
    "        if early_stopping.checkpoint_made:\n",
    "            checkpoint_train_loss, checkpoint_val_loss, checkpoint_test_loss, \\\n",
    "            checkpoint_train_acc, checkpoint_val_acc,  checkpoint_test_acc, \\\n",
    "            checkpoint_train_auc, checkpoint_val_auc,  checkpoint_test_auc, \\\n",
    "            checkpoint_train_auc_indiv, checkpoint_val_auc_indiv, checkpoint_test_auc_indiv, \\\n",
    "            checkpoint_train_prec, checkpoint_val_prec, checkpoint_test_prec,  \\\n",
    "            checkpoint_train_prec_indiv, checkpoint_val_prec_indiv, checkpoint_test_prec_indiv, \\\n",
    "            checkpoint_train_recall, checkpoint_val_recall, checkpoint_test_recall, \\\n",
    "            checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, checkpoint_test_recall_indiv, \\\n",
    "            checkpoint_train_f1, checkpoint_val_f1, checkpoint_test_f1, \\\n",
    "            checkpoint_train_f1_indiv, checkpoint_val_f1_indiv, checkpoint_test_f1_indiv = early_stopping.print_checkpoint_metric(train_loss_list, \n",
    "                                                                        val_loss_list, test_loss_list, train_acc_list, val_acc_list, \n",
    "                                                                        test_acc_list, train_auc_list, val_auc_list, test_auc_list, \n",
    "                                                                        train_auc_indiv_ave, val_auc_indiv_ave, test_auc_indiv_ave,\n",
    "                                                                        train_prec_list, val_prec_list, test_prec_list,\n",
    "                                                                        train_prec_indiv_ave, val_prec_indiv_ave, test_prec_indiv_ave,                                        \n",
    "                                                                        train_recall_list, val_recall_list, test_recall_list,\n",
    "                                                                        train_recall_indiv_ave, val_recall_indiv_ave, test_recall_indiv_ave,                                        \n",
    "                                                                        train_f1_list, val_f1_list, test_f1_list,\n",
    "                                                                        train_f1_indiv_ave, val_f1_indiv_ave, test_f1_indiv_ave)\n",
    "\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if variable_gamma:\n",
    "        gamma_val = float(model.tg_conv_layer1.gammat.numpy()) # gamma doesn't seem to be training correctly atm\n",
    "        print(gamma_val)\n",
    "    else:\n",
    "        gamma_val = 'N/A'\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    time_taken = timedelta(seconds=end_time - start_time)\n",
    "\n",
    "    list_for_csv = [run_name+\"_\"+str(ran_search_num), nepochs, epoch+1, str(time_taken), lr, out_chans, filter_size, \n",
    "                    lstm_h, reg_strength,\n",
    "                   linear_size, drop_val, train_batch_size, val_batch_size, test_batch_size,\n",
    "                    \n",
    "                    min(train_loss_epochs), max(train_loss_epochs), min(val_loss_epochs), \n",
    "                    max(val_loss_epochs), min(test_loss_epochs), max(test_loss_epochs),\n",
    "                    checkpoint_train_loss, checkpoint_val_loss, checkpoint_test_loss,                    \n",
    "\n",
    "                    min(train_auc_epochs), max(train_auc_epochs), min(val_auc_epochs), \n",
    "                    max(val_auc_epochs), min(test_auc_epochs), max(test_auc_epochs),\n",
    "                    checkpoint_train_auc, checkpoint_val_auc, checkpoint_test_auc,\n",
    "                    checkpoint_train_auc_indiv, checkpoint_val_auc_indiv, checkpoint_test_auc_indiv,                    \n",
    "                    \n",
    "                    min(train_acc_epochs), max(train_acc_epochs), min(val_acc_epochs), \n",
    "                    max(val_acc_epochs), min(test_acc_epochs), max(test_acc_epochs),\n",
    "                    checkpoint_train_acc, checkpoint_val_acc, checkpoint_test_acc,\n",
    "                    \n",
    "                    min(train_prec_epochs), max(train_prec_epochs), min(val_prec_epochs), \n",
    "                    max(val_prec_epochs), min(test_prec_epochs), max(test_prec_epochs),\n",
    "                    checkpoint_train_prec, checkpoint_val_prec, checkpoint_test_prec,   \n",
    "                    checkpoint_train_prec_indiv, checkpoint_val_prec_indiv, checkpoint_test_prec_indiv,\n",
    "\n",
    "                    min(train_recall_epochs), max(train_recall_epochs), min(val_recall_epochs), \n",
    "                    max(val_recall_epochs), min(test_recall_epochs), max(test_recall_epochs),\n",
    "                    checkpoint_train_recall, checkpoint_val_recall, checkpoint_test_recall, \n",
    "                    checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, checkpoint_test_recall_indiv, \n",
    "                    \n",
    "                    min(train_f1_epochs), max(train_f1_epochs), min(val_f1_epochs), \n",
    "                    max(val_f1_epochs), min(test_f1_epochs), max(test_f1_epochs),\n",
    "                    checkpoint_train_f1, checkpoint_val_f1, checkpoint_test_f1, \n",
    "                    checkpoint_train_f1_indiv, checkpoint_val_f1_indiv, checkpoint_test_f1_indiv,\n",
    "                    \n",
    "                    gamma_val, activation_type, LSTM_str, exp_str, timestamp_str, weighted_loss_str, L1_str, L2_str,\n",
    "                    second_layer_str]\n",
    "                    \n",
    "\n",
    "\n",
    "    # with open('comparison_table_util.csv', 'a', newline='') as f_object:\n",
    "    #     writer_object = writer(f_object)\n",
    "    #     writer_object.writerow(list_for_csv)\n",
    "    #     f_object.close()\n",
    "\n",
    "\n",
    "    # # to save .npy files incase graph needs looking at\n",
    "    # with open(\"npy_metrics/metric_vals_\"+run_name+\"_\"+str(ran_search_num)+\".npy\", 'wb') as f:\n",
    "    #     np.save(f, np.array(train_loss_epochs))\n",
    "    #     np.save(f, np.array(val_loss_epochs))\n",
    "    #     np.save(f, np.array(test_loss_epochs))\n",
    "\n",
    "    #     np.save(f, np.array(train_acc_epochs))\n",
    "    #     np.save(f, np.array(val_acc_epochs))\n",
    "    #     np.save(f, np.array(test_acc_epochs))\n",
    "        \n",
    "    #     np.save(f, np.array(train_auc_epochs))\n",
    "    #     np.save(f, np.array(val_auc_epochs))\n",
    "    #     np.save(f, np.array(test_auc_epochs)) \n",
    "    #     np.save(f, np.array(train_indiv_auc_epochs))\n",
    "    #     np.save(f, np.array(val_indiv_auc_epochs))\n",
    "    #     np.save(f, np.array(test_indiv_auc_epochs))\n",
    "\n",
    "    #     np.save(f, np.array(train_prec_epochs))\n",
    "    #     np.save(f, np.array(val_prec_epochs))\n",
    "    #     np.save(f, np.array(test_prec_epochs))\n",
    "    #     np.save(f, np.array(train_indiv_prec_epochs))\n",
    "    #     np.save(f, np.array(val_indiv_prec_epochs))\n",
    "    #     np.save(f, np.array(test_indiv_prec_epochs))\n",
    "        \n",
    "    #     np.save(f, np.array(train_recall_epochs))\n",
    "    #     np.save(f, np.array(val_recall_epochs))\n",
    "    #     np.save(f, np.array(test_recall_epochs))\n",
    "    #     np.save(f, np.array(train_indiv_recall_epochs))\n",
    "    #     np.save(f, np.array(val_indiv_recall_epochs))\n",
    "    #     np.save(f, np.array(test_indiv_recall_epochs))\n",
    "        \n",
    "    #     np.save(f, np.array(train_f1_epochs))\n",
    "    #     np.save(f, np.array(val_f1_epochs))\n",
    "    #     np.save(f, np.array(test_f1_epochs))\n",
    "    #     np.save(f, np.array(train_indiv_f1_epochs))\n",
    "    #     np.save(f, np.array(val_indiv_f1_epochs))\n",
    "    #     np.save(f, np.array(test_indiv_f1_epochs))        \n",
    "\n",
    "        \n",
    "    # utils.plot_loss_curve(train_loss = train_loss_epochs, val_loss = val_loss_epochs, test_lost = test_loss_epochs, \n",
    "    #                 run_name=run_name, ran_search_num=ran_search_num)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab92f56",
   "metadata": {},
   "source": [
    "`WARNING:tensorflow:Gradients do not exist for variables ['3DCNN_Weights:0'] when minimizing the loss. If you're using 'model.compile()', did you forget to provide a 'loss' argument?` error is caused by one of the conv_layers not being used for a single stream model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a2bb0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Printing logits and model summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8902aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.716378Z",
     "start_time": "2022-12-20T15:08:41.716378Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be4292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.717373Z",
     "start_time": "2022-12-20T15:08:41.717373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984340a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.721359Z",
     "start_time": "2022-12-20T15:08:41.721359Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits_np = test_logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091085a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.722355Z",
     "start_time": "2022-12-20T15:08:41.722355Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(logits_np, columns=['High', 'Low', 'Zero'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c0f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.722355Z",
     "start_time": "2022-12-20T15:08:41.722355Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Max'] = df.idxmax(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e77d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.723365Z",
     "start_time": "2022-12-20T15:08:41.723365Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Max.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1b6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "483.316px",
    "left": "1447.26px",
    "right": "20px",
    "top": "158.896px",
    "width": "529.462px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
