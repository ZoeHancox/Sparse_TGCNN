{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609f696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.694837Z",
     "start_time": "2022-12-20T15:00:03.051041Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import keras\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from src import utils, trnvaltst_sigmoid_oned, TGCNN_layer, whole_model, whole_model_demographics, create_fake_patients, plot_figures\n",
    "from early_stopping_cv import EarlyStopping\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, roc_auc_score, recall_score\n",
    "from csv import writer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "print(\"tensorflow version:\", tf. __version__)\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f64909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:00:24.709798Z",
     "start_time": "2022-12-20T15:00:24.696832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_LSTM = False # True = without LSTM\n",
    "exponential_scaling = True # True = with exponential\n",
    "L1_ablation = True # True = with L1 reg\n",
    "L2_ablation = True # True = with L2 reg\n",
    "variable_gamma = True\n",
    "graph_reg_incl = True\n",
    "\n",
    "num_of_runs = 1\n",
    "    \n",
    "weighted_loss = False # class weighted to deal with imbalance if True\n",
    "no_timestamp = False # if no_timestamp = True then all values in 3-tensor = 1\n",
    "activation_type = 'LeakyReLU' #'relu','gelu', 'LeakyReLU'\n",
    "second_TGCNN_layer = True\n",
    "demo = True\n",
    "include_drugs = False\n",
    "\n",
    "run_name='hip_1999_to_one_year_advance_model'\n",
    "\n",
    "# strings for hyperparameter searching file\n",
    "LSTM_str=\"LSTM excluded\" if no_LSTM == True else \"LSTM included\"\n",
    "exp_str = \"exp excluded\" if exponential_scaling == False else \"exp included\"\n",
    "timestamp_str = \"time elapsed = 1\" if no_timestamp == True else \"time elapsed\"\n",
    "weighted_loss_str = \"weighted_loss\" if weighted_loss ==True else \"unweighted_loss\"\n",
    "L1_str = \"L1 included\" if L1_ablation == True else \"L1 excluded\"\n",
    "L2_str = \"L2 included\" if L2_ablation == True else \"L2 excluded\"\n",
    "second_layer_str = \"Branched model\" if second_TGCNN_layer == True else \"Unbranched model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3eb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_drugs:\n",
    "    max_event_codes = 518\n",
    "else:\n",
    "    max_event_codes = 512\n",
    "num_labels = 1 # number of outcomes -1 \n",
    "hip_or_knee = 'hip'\n",
    "class_weights = tf.compat.v2.constant([[0.5, 0.5]]) # 50:50 split for the class weights\n",
    "\n",
    "# This would be where you would import your data instead of using the `create_fake_patient_df` function\n",
    "    # Data to train the model on \n",
    "cv_patients = create_fake_patients.create_fake_patient_df(num_patients=9000, max_events=100, max_nodes=512)\n",
    "    # Data to test and relicibrate on\n",
    "test_patients = create_fake_patients.create_fake_patient_df(num_patients=2000, max_events=100, max_nodes=512)\n",
    "    # Data to test the recalibrated model on\n",
    "recal_test_patients = create_fake_patients.create_fake_patient_df(num_patients=2000, max_events=100, max_nodes=512)\n",
    "\n",
    "cv_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_sample_size = len(cv_patients)\n",
    "test_sample_size = len(test_patients)\n",
    "recal_test_sample_size = len(recal_test_patients)\n",
    "\n",
    "# NOTE: num_time_steps isn't actually the number of time_steps it's the length of the values list (sometimes multiple Read codes will be reported in one visit)\n",
    "# This is needed to create a sparse matrix\n",
    "\n",
    "max_timesteps_cv = cv_patients['num_time_steps'].max() \n",
    "max_timesteps_test = test_patients['num_time_steps'].max()\n",
    "max_timesteps_recal = recal_test_patients['num_time_steps'].max()\n",
    "max_timesteps = max(max_timesteps_cv, max_timesteps_test, max_timesteps_recal)\n",
    "max_timesteps\n",
    "max_timesteps = max_timesteps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_y = utils.get_labels_binary(cv_patients)\n",
    "test_y = utils.get_labels_binary(test_patients)\n",
    "recal_test_y = utils.get_labels_binary(recal_test_patients)\n",
    "\n",
    "utils.check_group_sizes(cv_patients, test_patients, cv_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc65cc6",
   "metadata": {},
   "source": [
    "#### Convert list of lists of event codes into Sparse Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_input_matrices = utils.create_sparse_tensors(cv_sample_size, cv_patients, max_event_codes, max_timesteps, 'cv')\n",
    "test_input_matrices = utils.create_sparse_tensors(test_sample_size, test_patients, max_event_codes, max_timesteps, 'test')\n",
    "recal_test_input_matrices = utils.create_sparse_tensors(recal_test_sample_size, recal_test_patients, max_event_codes, max_timesteps, 'recal_test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72877dff",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning, model 5-fold-cv training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14366ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T15:08:41.699086Z",
     "start_time": "2022-12-20T15:04:46.128962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_size = int(round(cv_sample_size*0.2)) # 1/5th of cv data\n",
    "holdout_size = len(test_patients) # this contains 10% of the controls + 7 cases for each of these controls\n",
    "\n",
    "start_num = 1\n",
    "for ran_search_num in range(start_num, start_num+num_of_runs):\n",
    "    print('Run name:', run_name, ran_search_num)\n",
    "    cv_indices = list(range(len(cv_input_matrices)))\n",
    "    test_indices = list(range(len(test_input_matrices)))\n",
    "    recal_test_indices = list(range(len(recal_test_input_matrices)))\n",
    "\n",
    "    # Shuffle datasets to feed randomly into the model\n",
    "    random.shuffle(cv_indices)\n",
    "    random.shuffle(test_indices)\n",
    "    random.shuffle(recal_test_indices)\n",
    "    \n",
    "    # split the cv data for cross validation\n",
    "    cv_split1_indices = cv_indices[:cv_size]\n",
    "    cv_split2_indices = cv_indices[cv_size:2*cv_size]\n",
    "    cv_split3_indices = cv_indices[2*cv_size:3*cv_size]\n",
    "    cv_split4_indices = cv_indices[3*cv_size:4*cv_size]\n",
    "    cv_split5_indices = cv_indices[4*cv_size:5*cv_size]\n",
    "\n",
    "\n",
    "    batch_size = 128 # choose a batch size\n",
    "\n",
    "    batched_graphs_split1, batched_labels_split1, batched_cv_split1_indices = utils.batch_set(indice_set=cv_split1_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split2, batched_labels_split2, batched_cv_split2_indices = utils.batch_set(indice_set=cv_split2_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split3, batched_labels_split3, batched_cv_split3_indices = utils.batch_set(indice_set=cv_split3_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split4, batched_labels_split4, batched_cv_split4_indices = utils.batch_set(indice_set=cv_split4_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    batched_graphs_split5, batched_labels_split5, batched_cv_split5_indices = utils.batch_set(indice_set=cv_split5_indices, input_matrices=cv_input_matrices, \n",
    "                                                       labels=cv_y.to_numpy(), batchsize=batch_size)\n",
    "    \n",
    "    \n",
    "    batched_graphs_holdout, batched_labels_holdout, batched_holdout_indices = utils.batch_set(indice_set=test_indices, input_matrices=test_input_matrices, \n",
    "                                                         labels=test_y.to_numpy(), batchsize=batch_size)\n",
    "    labels_holdout = [item for sublist in batched_labels_holdout for item in sublist]\n",
    "    \n",
    "    batched_graphs_holdout2, batched_labels_holdout2, batched_holdout2_indices = utils.batch_set(indice_set=recal_test_indices, input_matrices=recal_test_input_matrices, \n",
    "                                                         labels=recal_test_y.to_numpy(), batchsize=batch_size)\n",
    "    labels_holdout2 = [item for sublist in batched_labels_holdout2 for item in sublist]\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    # Save the true labels for analysis later\n",
    "    file_full_name_true = 'pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout1_true.npy'\n",
    "    with open(file_full_name_true, 'wb') as f:\n",
    "        np.save(f, labels_holdout)\n",
    "        \n",
    "    file_full_name_true2 = 'pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout2_true.npy'\n",
    "    with open(file_full_name_true2, 'wb') as f:\n",
    "        np.save(f, labels_holdout2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # split cv data into batches for training\n",
    "    graphs_train1, graphs_train2, graphs_train3, graphs_train4, graphs_train5, graphs_test1, graphs_test2, \\\n",
    "    graphs_test3, graphs_test4, graphs_test5, labels_train1, labels_train2, labels_train3, labels_train4, \\\n",
    "    labels_train5, labels_test1, labels_test2, labels_test3, labels_test4, labels_test5, indices_train1, \\\n",
    "    indices_train2, indices_train3, indices_train4, indices_train5, indices_test1, indices_test2, indices_test3, \\\n",
    "    indices_test4, indices_test5, \\\n",
    "    = utils.get_cv_groups(batched_graphs_split1, batched_graphs_split2, batched_graphs_split3, batched_graphs_split4, \n",
    "                    batched_graphs_split5, batched_labels_split1, batched_labels_split2, batched_labels_split3, \n",
    "                    batched_labels_split4, batched_labels_split5, batched_cv_split1_indices, batched_cv_split2_indices, \n",
    "                    batched_cv_split3_indices, batched_cv_split4_indices, batched_cv_split5_indices)\n",
    "\n",
    "    \n",
    "    nepochs=20\n",
    "    # Random hyperparameter selection\n",
    "    lr = random.choice([0.001, 0.005, 0.0001]) # initial learning rate\n",
    "\n",
    "    out_chans = random.choice([8, 16, 32]) # number of filters\n",
    "    filter_size = random.choice([3, 4, 6]) # number of time steps/ filter size\n",
    "    if no_LSTM:\n",
    "        lstm_h = 0\n",
    "    else:\n",
    "        lstm_h = random.choice([16, 32, 64, 128, 256]) # number of LSTM neurons\n",
    "    linear_size = random.choice([64, 128]) # linear layer output size\n",
    "    drop_val = random.choice([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "    reg_strength = random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\n",
    "    graph_reg_strength = 1e1\n",
    "    \n",
    "\n",
    "    print(f\"Number of epochs: {nepochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Number of 3D CNN filters: {out_chans}\")\n",
    "    print(f\"Filter size: {filter_size}\")\n",
    "    print(f\"Number of LSTM neurons: {lstm_h}\")\n",
    "    print(f\"Number of fully connected layers: {linear_size}\")\n",
    "    print(f\"Dropout value: {drop_val}\")\n",
    "    print(f\"Regularisation strength: {reg_strength}\")\n",
    "    print(\"*\"*40)\n",
    "    \n",
    "\n",
    "    #############################################\n",
    "    ####### FOR LOOP FOR CV HERE  ###############\n",
    "    #############################################\n",
    "    \n",
    "    cv_accuracy, cv_auc, cv_cal_slope = [], [], []\n",
    "    best_metric = 0\n",
    "    for split in range(0, 5):\n",
    "        run_split_name = run_name +\"_split\" + str(split+1)\n",
    "        print(run_split_name, \"_find\")\n",
    "        \n",
    "        if demo:\n",
    "            model = whole_model_demographics.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                            exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                            fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                            no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer, num_labels=num_labels)\n",
    "        else:\n",
    "            model = whole_model.TGCNN_Model(num_filters=out_chans, num_nodes=max_event_codes, num_time_steps=max_timesteps, \n",
    "                            filter_size=filter_size, variable_gamma=variable_gamma, \n",
    "                            exponential_scaling=exponential_scaling, dropout_rate=drop_val, lstm_units=lstm_h,\n",
    "                            fcl1_units=linear_size, LSTM_ablation=no_LSTM, stride=1, activation_type=activation_type, \n",
    "                            no_timestamp=no_timestamp, second_TGCNN_layer=second_TGCNN_layer, num_labels=num_labels)\n",
    "    \n",
    "        early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=lr,\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        if split+1 == 1:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train1, labels_train1\n",
    "            batched_graphs_val, batched_labels_val = graphs_test1, labels_test1\n",
    "            train_indices = indices_train1\n",
    "            test_indices = indices_test1\n",
    "            \n",
    "        elif split+1 == 2:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train2, labels_train2\n",
    "            batched_graphs_val, batched_labels_val = graphs_test2, labels_test2\n",
    "            train_indices = indices_train2\n",
    "            test_indices = indices_test2\n",
    "            \n",
    "        elif split+1 == 3:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train3, labels_train3\n",
    "            batched_graphs_val, batched_labels_val = graphs_test3, labels_test3\n",
    "            train_indices = indices_train3\n",
    "            test_indices = indices_test3\n",
    "            \n",
    "        elif split+1 == 4:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train4, labels_train4\n",
    "            batched_graphs_val, batched_labels_val = graphs_test4, labels_test4\n",
    "            train_indices = indices_train4\n",
    "            test_indices = indices_test4\n",
    "            \n",
    "        elif split+1 == 5:\n",
    "            batched_graphs_trn, batched_labels_trn = graphs_train5, labels_train5\n",
    "            batched_graphs_val, batched_labels_val = graphs_test5, labels_test5\n",
    "            train_indices = indices_train5\n",
    "            test_indices = indices_test5\n",
    "            \n",
    "        \n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss_epochs, train_acc_epochs, train_prec_epochs, train_recall_epochs, train_f1_epochs, train_auc_epochs = [],[],[],[],[],[]\n",
    "        train_indiv_acc_epochs, train_indiv_prec_epochs, train_indiv_recall_epochs, train_indiv_f1_epochs, train_indiv_auc_epochs = [],[],[],[],[]\n",
    "        val_loss_epochs, val_acc_epochs, val_prec_epochs, val_recall_epochs, val_f1_epochs, val_auc_epochs = [],[],[],[],[],[]\n",
    "        val_indiv_acc_epochs, val_indiv_prec_epochs, val_indiv_recall_epochs, val_indiv_f1_epochs, val_indiv_auc_epochs = [],[],[],[],[]\n",
    "        \n",
    "        for epoch in range(nepochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{nepochs}\")\n",
    "\n",
    "            # Iterate over the batches of the dataset.\n",
    "            train_loss_list, train_acc_list, train_prec_list, train_recall_list, train_f1_list, train_auc_list = [],[],[],[],[],[]\n",
    "            train_cal_slope_list = []\n",
    "            train_all_classes_prec_list, train_all_classes_recall_list, train_all_classes_f1_list, train_all_classes_auc_list = [],[],[],[] # lists of list\n",
    "\n",
    "            for x_batch_train, y_batch_train, train_indices_list in zip(batched_graphs_trn, batched_labels_trn, train_indices):\n",
    "            # need to get one list at a time from the list of lists\n",
    "                    \n",
    "                trn_demo_vals, trn_demo_list = utils.convert_demos_to_tensor(cv_patients, train_indices_list, demo)\n",
    "                \n",
    "                if demo == False:\n",
    "                    trn_demo_vals = None\n",
    "                \n",
    "                y_batch_train=np.array(y_batch_train)\n",
    "                trn_logits, trn_loss, trn_acc, trn_prec, trn_recall, trn_auc, trn_f1, \\\n",
    "                indiv_trn_prec, indiv_trn_recall, indiv_trn_auc, indiv_trn_f1, trn_cal_slope, model,\\\n",
    "                = trnvaltst_sigmoid_oned.train_step(x_batch_train, y_batch_train, trn_demo_vals,\n",
    "                                                        reg_strength,class_weights,model,L1_ablation,L2_ablation, \n",
    "                                                       graph_reg_strength, graph_reg_incl, exponential_scaling,\n",
    "                                                       weighted_loss, variable_gamma, optimizer, demo)\n",
    "\n",
    "                train_loss_list.append(trn_loss)\n",
    "                train_cal_slope_list.append(trn_cal_slope)\n",
    "                train_acc_list.append(trn_acc)\n",
    "\n",
    "                train_all_classes_prec_list.append(trn_prec)\n",
    "                train_prec_list.append(indiv_trn_prec)\n",
    "\n",
    "                train_all_classes_recall_list.append(trn_recall)\n",
    "                train_recall_list.append(indiv_trn_recall)\n",
    "\n",
    "                train_all_classes_auc_list.append(trn_auc)\n",
    "                train_auc_list.append(indiv_trn_auc)\n",
    "\n",
    "                train_all_classes_f1_list.append(trn_f1)\n",
    "                train_f1_list.append(indiv_trn_f1)\n",
    "\n",
    "            if variable_gamma and (exponential_scaling==True):\n",
    "                gamma_val = float(model.tg_conv_layer1.gammat.numpy())\n",
    "                print(\"Gamma:\", gamma_val)\n",
    "                if second_TGCNN_layer:\n",
    "                    gamma_val2 = float(model.tg_conv_layer2.gammat.numpy())\n",
    "                    print(\"Gamma 2:\", gamma_val2)\n",
    "\n",
    "            # get the average metric score for each class for this ONE epoch using the batch list metrics\n",
    "            train_prec_indiv_ave = utils.average_of_list_of_lists(train_all_classes_prec_list)\n",
    "            train_recall_indiv_ave = utils.average_of_list_of_lists(train_all_classes_recall_list)\n",
    "            train_f1_indiv_ave = utils.average_of_list_of_lists(train_all_classes_f1_list) # ave F1 score for each class\n",
    "            train_auc_indiv_ave = utils.average_of_list_of_lists(train_all_classes_auc_list)\n",
    "\n",
    "            print(\"\\nTRAINING METRICS:\")\n",
    "            print(f\"Training AUC: {np.mean(train_auc_list):.4f}\")\n",
    "            print(f\"Train loss {np.mean(train_loss_list):.4f}\")\n",
    "            print(f\"Train calibration slope {np.mean(train_cal_slope_list):.4f}\")\n",
    "            print(f\"Train accuracy: {np.mean(train_acc_list) :.4%}\")\n",
    "\n",
    "\n",
    "            # Validation loop\n",
    "            val_loss_list, val_acc_list,  val_prec_list, val_recall_list, val_auc_list, val_f1_list = [],[],[],[],[],[]\n",
    "            val_cal_slope_list = []\n",
    "            val_all_classes_acc_list, val_all_classes_prec_list, val_all_classes_recall_list, val_all_classes_f1_list, val_all_classes_auc_list = [],[],[],[],[]\n",
    "            for x_batch_val, y_batch_val, test_indices_list in zip(batched_graphs_val, batched_labels_val, test_indices):\n",
    "                \n",
    "                test_demo_vals, test_demo_list = utils.convert_demos_to_tensor(cv_patients, test_indices_list, demo)                \n",
    "                \n",
    "                if demo == False:\n",
    "                    test_demo_vals = None\n",
    "                \n",
    "                val_logits, val_loss, val_acc, val_prec, val_recall, val_auc, val_f1, \\\n",
    "                indiv_val_prec, indiv_val_recall, indiv_val_auc, indiv_val_f1, val_cal_slope, model, \\\n",
    "                = trnvaltst_sigmoid_oned.val_step(x_batch_val, y_batch_val, test_demo_vals, reg_strength, \n",
    "                                                  class_weights, model, L1_ablation, L2_ablation, \n",
    "                                                  graph_reg_strength, graph_reg_incl, weighted_loss, demo)\n",
    "\n",
    "                val_loss_list.append(val_loss)\n",
    "                val_cal_slope_list.append(val_cal_slope)\n",
    "\n",
    "                val_acc_list.append(val_acc)\n",
    "\n",
    "                val_prec_list.append(val_prec)\n",
    "                val_all_classes_prec_list.append(indiv_val_prec)\n",
    "\n",
    "                val_recall_list.append(val_recall) \n",
    "                val_all_classes_recall_list.append(indiv_val_recall)\n",
    "\n",
    "                val_auc_list.append(val_auc)\n",
    "                val_all_classes_auc_list.append(indiv_val_auc)\n",
    "\n",
    "                val_f1_list.append(val_f1)\n",
    "                val_all_classes_f1_list.append(indiv_val_f1)\n",
    "            \n",
    "            plot_figures.draw_confusion_mat(y_batch_val, val_logits, ['none','hip'], run_name=None, ran_search_num=1111, data_type=\"V\")\n",
    "            plot_figures.draw_calibration_curve(y_batch_val, val_logits, run_name=None, ran_search_num=1111)\n",
    "            \n",
    "            \n",
    "            val_prec_indiv_ave = utils.average_of_list_of_lists(val_all_classes_prec_list)\n",
    "            val_recall_indiv_ave = utils.average_of_list_of_lists(val_all_classes_recall_list)\n",
    "            val_f1_indiv_ave = utils.average_of_list_of_lists(val_all_classes_f1_list) # ave F1 score for each class\n",
    "            val_auc_indiv_ave = utils.average_of_list_of_lists(val_all_classes_auc_list)\n",
    "            \n",
    "            # Saves the logits if the AUC improves\n",
    "            improved_metric = False\n",
    "            if np.mean(val_auc_list) > best_metric:\n",
    "                best_metric = np.mean(val_auc_list)\n",
    "                improved_metric = True\n",
    "\n",
    "            print(\"\\nVALIDATION METRICS:\")\n",
    "            print(f\"Validation AUC: {np.mean(val_auc_list):.4f}\")\n",
    "            print(f\"Validation loss: {np.mean(val_loss_list):.4f}\")\n",
    "            print(f\"Validation calibration slope: {np.mean(val_cal_slope_list):.4f}\")\n",
    "            print(f\"Validation accuracy: {np.mean(val_acc_list) :.4%}\")\n",
    "\n",
    "\n",
    "\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # get the average metric from ONE epoch\n",
    "            ave_epoch_train_loss, ave_epoch_train_acc, ave_epoch_train_auc, ave_epoch_train_prec, ave_epoch_train_recall, ave_epoch_train_f1 = np.mean(train_loss_list), np.mean(train_acc_list), np.mean(train_auc_list), np.mean(train_prec_list), np.mean(train_recall_list), np.mean(train_f1_list)\n",
    "            ave_epoch_val_loss, ave_epoch_val_acc, ave_epoch_val_auc, ave_epoch_val_prec, ave_epoch_val_recall, ave_epoch_val_f1 = np.mean(val_loss_list), np.mean(val_acc_list), np.mean(val_auc_list), np.mean(val_prec_list), np.mean(val_recall_list), np.mean(val_f1_list)        \n",
    "\n",
    "\n",
    "            #print(optimizer.get_config())\n",
    "\n",
    "            # save the average (from one epoch) to the list of ALL epochs\n",
    "            utils.metric_save(ave_epoch_train_loss, train_loss_epochs, ave_epoch_val_loss, val_loss_epochs) #losses\n",
    "            utils.metric_save(ave_epoch_train_acc, train_acc_epochs, ave_epoch_val_acc, val_acc_epochs) # acc\n",
    "            utils.metric_save(ave_epoch_train_auc, train_auc_epochs, ave_epoch_val_auc, val_auc_epochs) # auc        \n",
    "            utils.metric_save(ave_epoch_train_prec, train_prec_epochs, ave_epoch_val_prec, val_prec_epochs) # prec\n",
    "            utils.metric_save(ave_epoch_train_recall, train_recall_epochs, ave_epoch_val_recall, val_recall_epochs) # recall\n",
    "            utils.metric_save(ave_epoch_train_f1, train_f1_epochs, ave_epoch_val_f1, val_f1_epochs) #f1\n",
    "\n",
    "            # save the average from one epoch to the list of ALL epochs for each individual class\n",
    "            utils.metric_save(train_auc_indiv_ave, train_indiv_auc_epochs, val_auc_indiv_ave, val_indiv_auc_epochs)\n",
    "            utils.metric_save(train_prec_indiv_ave, train_indiv_prec_epochs, val_prec_indiv_ave, val_indiv_prec_epochs)\n",
    "            utils.metric_save(train_recall_indiv_ave, train_indiv_recall_epochs, val_recall_indiv_ave, val_indiv_recall_epochs)\n",
    "            utils.metric_save(train_f1_indiv_ave, train_indiv_f1_epochs, val_f1_indiv_ave, val_indiv_f1_epochs)\n",
    "\n",
    "\n",
    "            early_stopping_metric = val_loss_list # metric that is used to determine if the model should stop training\n",
    "            early_stopping(np.mean(early_stopping_metric), train_loss_list, val_loss_list, \n",
    "                           train_cal_slope_list, val_cal_slope_list,\n",
    "                           train_acc_list, val_acc_list,  \n",
    "                           train_auc_list, val_auc_list,\n",
    "                           train_auc_indiv_ave, val_auc_indiv_ave,\n",
    "                           train_prec_list, val_prec_list,\n",
    "                           train_prec_indiv_ave, val_prec_indiv_ave,\n",
    "                           train_recall_list, val_recall_list,\n",
    "                           train_recall_indiv_ave, val_recall_indiv_ave,\n",
    "                           train_f1_list, val_f1_list,\n",
    "                           train_f1_indiv_ave, val_f1_indiv_ave, \n",
    "                           model, run_name+str(ran_search_num),\n",
    "                           batched_graphs_holdout, batched_labels_holdout, batched_holdout_indices,\n",
    "                           batched_graphs_holdout2, batched_labels_holdout2, batched_holdout2_indices,\n",
    "                           reg_strength,\n",
    "                           class_weights, L1_ablation, L2_ablation, graph_reg_strength, graph_reg_incl, weighted_loss, \n",
    "                           improved_metric, test_patients, recal_test_patients, \n",
    "                           demo)\n",
    "\n",
    "            if early_stopping.checkpoint_made:\n",
    "\n",
    "                checkpoint_train_loss, checkpoint_val_loss, \\\n",
    "                checkpoint_train_cal_slope, checkpoint_val_cal_slope, \\\n",
    "                checkpoint_train_acc, checkpoint_val_acc, \\\n",
    "                checkpoint_train_auc, checkpoint_val_auc, \\\n",
    "                checkpoint_train_auc_indiv, checkpoint_val_auc_indiv, \\\n",
    "                checkpoint_train_prec, checkpoint_val_prec,  \\\n",
    "                checkpoint_train_prec_indiv, checkpoint_val_prec_indiv, \\\n",
    "                checkpoint_train_recall, checkpoint_val_recall, \\\n",
    "                checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, \\\n",
    "                checkpoint_train_f1, checkpoint_val_f1, \\\n",
    "                checkpoint_train_f1_indiv, checkpoint_val_f1_indiv = early_stopping.print_checkpoint_metric(train_loss_list, \n",
    "                                                                            val_loss_list, \n",
    "                                                                            train_cal_slope_list, val_cal_slope_list,\n",
    "                                                                            train_acc_list, val_acc_list, \n",
    "                                                                            train_auc_list, val_auc_list, \n",
    "                                                                            train_auc_indiv_ave, val_auc_indiv_ave,\n",
    "                                                                            train_prec_list, val_prec_list,\n",
    "                                                                            train_prec_indiv_ave, val_prec_indiv_ave,                                        \n",
    "                                                                            train_recall_list, val_recall_list,\n",
    "                                                                            train_recall_indiv_ave, val_recall_indiv_ave,                                        \n",
    "                                                                            train_f1_list, val_f1_list,\n",
    "                                                                            train_f1_indiv_ave, val_f1_indiv_ave)\n",
    "\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        if variable_gamma and (exponential_scaling==True):\n",
    "            gamma_val = float(model.tg_conv_layer1.gammat.numpy())\n",
    "            print(\"Gamma:\", gamma_val)\n",
    "            if second_TGCNN_layer:\n",
    "                gamma_val2 = float(model.tg_conv_layer2.gammat.numpy())\n",
    "                print(\"Gamma 2:\", gamma_val2)\n",
    "                \n",
    "        else:\n",
    "            gamma_val = 'N/A'\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        time_taken = timedelta(seconds=end_time - start_time)\n",
    "\n",
    "        list_for_csv = [run_split_name+\"_\"+str(ran_search_num), nepochs, epoch+1, str(time_taken), lr, out_chans, filter_size, \n",
    "                        lstm_h, reg_strength,\n",
    "                       linear_size, drop_val, batch_size,\n",
    "\n",
    "                        min(train_loss_epochs), max(train_loss_epochs), min(val_loss_epochs), \n",
    "                        max(val_loss_epochs), checkpoint_train_loss, checkpoint_val_loss,                    \n",
    "\n",
    "                        min(train_auc_epochs), max(train_auc_epochs), min(val_auc_epochs), \n",
    "                        max(val_auc_epochs), checkpoint_train_auc, checkpoint_val_auc,\n",
    "                        checkpoint_train_auc_indiv, checkpoint_val_auc_indiv,                    \n",
    "\n",
    "                        min(train_acc_epochs), max(train_acc_epochs), min(val_acc_epochs), \n",
    "                        max(val_acc_epochs), checkpoint_train_acc, checkpoint_val_acc,\n",
    "\n",
    "                        min(train_prec_epochs), max(train_prec_epochs), min(val_prec_epochs), \n",
    "                        max(val_prec_epochs), checkpoint_train_prec, checkpoint_val_prec,   \n",
    "                        checkpoint_train_prec_indiv, checkpoint_val_prec_indiv,\n",
    "\n",
    "                        min(train_recall_epochs), max(train_recall_epochs), min(val_recall_epochs), \n",
    "                        max(val_recall_epochs), checkpoint_train_recall, checkpoint_val_recall, \n",
    "                        checkpoint_train_recall_indiv, checkpoint_val_recall_indiv, \n",
    "\n",
    "                        min(train_f1_epochs), max(train_f1_epochs), min(val_f1_epochs), \n",
    "                        max(val_f1_epochs), checkpoint_train_f1, checkpoint_val_f1, \n",
    "                        checkpoint_train_f1_indiv, checkpoint_val_f1_indiv,\n",
    "\n",
    "                        gamma_val, \n",
    "                        checkpoint_train_cal_slope, checkpoint_val_cal_slope,\n",
    "                        activation_type, LSTM_str, exp_str, timestamp_str, weighted_loss_str, L1_str, L2_str,\n",
    "                        second_layer_str]\n",
    "\n",
    "\n",
    "\n",
    "        with open('results/cross_validation_results.csv', 'a', newline='') as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(list_for_csv)\n",
    "            f_object.close()\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\nCHECKPOINTED RESULTS\")\n",
    "        print(f\"Train loss: {checkpoint_train_loss:.4f}\") # average loss from the epoch\n",
    "        print(f\"Train AUC: {checkpoint_train_auc:.4%}\\n\")\n",
    "        print(f\"Val loss: {checkpoint_val_loss:.4f}\") # average loss from the epoch\n",
    "        print(f\"Val AUC: {checkpoint_val_auc:.4%}\\n\")\n",
    "\n",
    "        print(\"=\"*50)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        cv_accuracy.append(checkpoint_val_acc)\n",
    "        cv_auc.append(checkpoint_val_auc)\n",
    "        cv_cal_slope.append(checkpoint_val_cal_slope)\n",
    "        \n",
    "    print(f\"5-fold CV Accuracy on (Validation Set): {np.mean(cv_accuracy):.4f} +/- {np.std(cv_accuracy):.4f}\")\n",
    "    print(f\"5-fold CV AUC on (Validation Set): {np.mean(cv_auc):.4f} +/- {np.std(cv_auc):.4f}\")\n",
    "    print(f\"5-fold CV Calibration Slope (Validation Set): {np.mean(cv_cal_slope):.4f} +/- {np.std(cv_cal_slope):.4f}\")\n",
    "    print(\"*\"*100)\n",
    "        \n",
    "    class_names = ['none', 'hip']\n",
    "#     plot_figures.plot_loss_curve(train_loss = train_loss_epochs, val_loss = val_loss_epochs, test_lost = test_loss_epochs, \n",
    "#                     run_name=run_split_name, ran_search_num=ran_search_num)\n",
    "\n",
    "\n",
    "    # open the proba file and then plot the figures for the test data\n",
    "    # The model is tested during early stopping\n",
    "    with open('pred_proba_and_true/'+run_name+str(ran_search_num)+'_holdout1_proba.npy', 'rb') as f:\n",
    "        test_logits = np.load(f)\n",
    "\n",
    "    plot_figures.draw_confusion_mat(labels_holdout, test_logits, class_names, run_name, ran_search_num)\n",
    "    plot_figures.draw_calibration_curve(labels_holdout, test_logits, run_name, ran_search_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "483.316px",
    "left": "1447.26px",
    "right": "20px",
    "top": "158.896px",
    "width": "529.462px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
