{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the scores/metrics given true y and the probabilities from the models (Test set 1 - pre-re-calibration)\n",
    "\n",
    "* Calculate:\n",
    "\n",
    "    * True Positive\n",
    "    * True Negative\n",
    "    * False Positive\n",
    "    * False Negative\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1\n",
    "    * Calibration Slope\n",
    "    \n",
    "* Generate:\n",
    "    * Histogram of probabilities\n",
    "    * Calibration Curve\n",
    "    * Logits and true test results for recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, auc, precision_recall_curve, roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_calibration_curve(true_y, pred_y, run_name):\n",
    "    \"\"\"\n",
    "    Plot a calibration curve for a binary model\n",
    "    \n",
    "    Args:\n",
    "        true_y (list): list of single integer arrays with 0 or 1 depending on the class\n",
    "        pred_y (logits tensor): logits probability of each class shape=(number_of_timestamps, 2)\n",
    "    Returns:\n",
    "        Matplotlib figure: Calibration curve showing mean predicted value vs fraction of positives\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #prob_pos = pred_y[:,1] # Change to get the probability of the positive class only\n",
    "    prob_pos = np.array(pred_y)\n",
    "\n",
    "    fraction_of_pos, mean_pred_value = calibration_curve(true_y, pred_y, n_bins=10)#, normalize=False)\n",
    "    \n",
    "    m, b = np.polyfit(mean_pred_value, fraction_of_pos, 1)\n",
    "\n",
    "    plt.plot(mean_pred_value, fraction_of_pos, '-', label='Overall Calibration')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', label='Ideal Calibration')\n",
    "    plt.title(f\"Calibration Curve Slope={m:.2f}\")\n",
    "    plt.xlabel('Mean Predicted Value')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_histo(probabilities):\n",
    "    #positive_class_probabilities = probabilities[:, 1]\n",
    "    sns.histplot(probabilities, bins=10, kde=True, color='#4CAF50', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title('Distribution of Probabilities', fontsize=16)\n",
    "    plt.xlabel('Probability', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim([0, 1])\n",
    "\n",
    "    # Customize tick labels\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgcnn_save_df_for_recal(logits, y_true, run_name, demo):\n",
    "    probs = 1 / (1 + (np.exp(-logits)))\n",
    "    prob_histo(probs)\n",
    "    draw_calibration_curve(y_true, probs, run_name)\n",
    "    df = pd.DataFrame.from_dict({'outcome': y_true, 'logit': logits})\n",
    "\n",
    "    num_rows = len(demo) // 3\n",
    "    # Reshape the list into a 2D array with 3 columns\n",
    "    reshaped_list = [demo[i:i+3] for i in range(0, len(demo), 3)]\n",
    "    demo_df = pd.DataFrame(reshaped_list, columns=['sex', 'imd_quint', 'age_at_label'])\n",
    "    demo_df['sex'] = demo_df['sex'].astype(int)\n",
    "    demo_df['imd_quint'] = demo_df['imd_quint'].astype(int)\n",
    "\n",
    "    df = pd.concat([df, demo_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def baseline_save_df_for_recal(probs, y_true, run_name):\n",
    "    prob_histo(probs)\n",
    "    draw_calibration_curve(y_true, probs, run_name)\n",
    "    logits = np.log(probs/(1.0000001-probs))\n",
    "    df = pd.DataFrame.from_dict({'outcome': y_true, 'logit': logits})\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat_and_scores(run_name, baseline=True):\n",
    "    \"\"\"\n",
    "    Draw a confusion matrix using the sklearn.metrics confusion matrix\n",
    "    package.\n",
    "    \n",
    "    Args:       \n",
    "        run_name: name of the file (str) without the suffix of 'proba' or 'true'\n",
    "    Returns:\n",
    "        sns confusion matrix plot.\n",
    "    \"\"\"\n",
    "    print(run_name.upper())\n",
    "    \n",
    "    folder = \"../pred_proba_and_true/\"\n",
    "\n",
    "    results_proba_loc1 = folder + run_name + \"_holdout1_proba.npy\"\n",
    "    true_test_loc1 = folder + run_name + \"_holdout1_true.npy\"\n",
    "    \n",
    "    results_proba_loc2 = folder + run_name + \"_holdout2_proba.npy\"\n",
    "    true_test_loc2 = folder + run_name + \"_holdout2_true.npy\"\n",
    "\n",
    "    if baseline == False:\n",
    "        demo_loc1 = folder + run_name + \"_holdout1_demo.npy\"\n",
    "        demo_loc2 = folder + run_name + \"_holdout2_demo.npy\"\n",
    "        \n",
    "        \n",
    "\n",
    "    with open(results_proba_loc1, 'rb') as f:\n",
    "        results_proba1 = np.load(f)\n",
    "    with open(true_test_loc1, 'rb') as f:\n",
    "        true_test1 = np.load(f, allow_pickle=True)\n",
    "    \n",
    "\n",
    "    with open(results_proba_loc2, 'rb') as f:\n",
    "        results_proba2 = np.load(f)\n",
    "    with open(true_test_loc2, 'rb') as f:\n",
    "        true_test2 = np.load(f, allow_pickle=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    #class_names = ['None', 'Hip']\n",
    "    y_true = np.array(true_test1).squeeze() # change from 2D array to 1D in row instead of col vec\n",
    "\n",
    "    \n",
    "    if baseline == False:\n",
    "        probs = results_proba1\n",
    "        y_pred = np.where(probs > 0.5, 1, 0) \n",
    "        \n",
    "        with open(demo_loc1, 'rb') as f:\n",
    "            demo1 = np.load(f, allow_pickle=True)\n",
    "        with open(demo_loc2, 'rb') as f:\n",
    "            demo2 = np.load(f, allow_pickle=True)\n",
    "\n",
    "    else:\n",
    "        if 'LSTM' in run_name or 'RNN' in run_name:\n",
    "            results_proba1 = results_proba1[:, 1] # get the column for the positive class\n",
    "            results_proba2 = results_proba2[:, 1] # get the column for the positive class\n",
    "        probs = results_proba1\n",
    "        y_pred = np.where(probs > 0.5, 1, 0) \n",
    "\n",
    "\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    sns.heatmap(cm,annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "#     tick_marks = np.arange(len(class_names)) + 0.5\n",
    "#     plt.yticks(tick_marks, class_names)\n",
    "#     plt.xticks(tick_marks, class_names)\n",
    "    plt.title(\"Confusion Matrix for \"+run_name)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    # F1 Score\n",
    "    F1 = 2*((PPV*TPR)/(PPV+TPR))\n",
    "    \n",
    "    model_precision, model_recall, _ = precision_recall_curve(y_true, results_proba1)\n",
    "    \n",
    "    # AUROC\n",
    "    AUROC = roc_auc_score(y_true, probs)\n",
    "    \n",
    "    \n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, probs)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    # AUPRC\n",
    "    AUPRC = auc(model_recall, model_precision)\n",
    "    \n",
    "    print(f\"True Positive: {TP}\")\n",
    "    print(f\"True Negative: {TN}\")\n",
    "    print(f\"False Positive: {FP}\")\n",
    "    print(f\"False Negative: {FN}\")\n",
    "    print(f\"\\nSensitivity/Recall: {TPR:.3f}\")\n",
    "    print(f\"Specificity: {TNR:.3f}\")\n",
    "    print(f\"Precision/Positive Predictive Value: {PPV:.3f}\")\n",
    "    print(f\"F1 Score: {F1:.3f}\")\n",
    "    print(f\"\\nAccuracy: {ACC:.3f}\")\n",
    "    print(f\"AUROC: {AUROC:.3f}\")\n",
    "    print(f\"AUPRC: {AUPRC:.3f}\")\n",
    "    \n",
    "    \n",
    "    # Save numpy logits and true values as csv for recalibration in R\n",
    "    # Also save the test set 2 for testing recalibration\n",
    "    if baseline == True:\n",
    "               \n",
    "        df1 = baseline_save_df_for_recal(results_proba1, y_true, run_name)\n",
    "        df2 = baseline_save_df_for_recal(results_proba2, np.array(true_test2).squeeze(), run_name)\n",
    "    else:        \n",
    "        df1 = tgcnn_save_df_for_recal(results_proba1, y_true, run_name, demo1)\n",
    "        df2 = tgcnn_save_df_for_recal(results_proba2, np.array(true_test2).squeeze(), run_name, demo2)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    df1.to_csv(\"../recalibration/logits_and_outcome_csvs/logits_\"+run_name+\"_holdout_1.csv\")\n",
    "    df2.to_csv(\"../recalibration/logits_and_outcome_csvs/logits_\"+run_name+\"_holdout_2.csv\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get results and logits and outcomes for recalibration in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TGCNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat_and_scores(run_name=\"hip_1999_to_one_year_advance_model1\", baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat_and_scores(run_name=\"LR_only_read_codes_1999_to_one_year_L2\", baseline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
